
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ğŸ“š https://github.com/karpathy/nanogpt - Code Repository</title>

    <!-- PWA Manifest -->
    <link rel="manifest" href="data:application/manifest+json;base64,eyJuYW1lIjogIlJlcG9zaXRvcnkgRXhwbG9yZXIiLCAic2hvcnRfbmFtZSI6ICJSZXBvRXhwbG9yZXIiLCAiaWNvbnMiOiBbeyJzcmMiOiAiZGF0YTppbWFnZS9zdmcreG1sO2Jhc2U2NCxQSE4yWnlCNGJXeHVjejBpYUhSMGNEb3ZMM2QzZHk1M015NXZjbWN2TWpBd01DOXpkbWNpSUhacFpYZENiM2c5SWpBZ01DQXhNekFnTVRNd0lpQjNhV1IwYUQwaU1UTXdJaUJvWldsbmFIUTlJakV6TUNJOGMzUjViR1UrSUc1aGJXVTlJbU52Y0dGamFYUjVJaUJtYVd4c1BTSWpaRFl5TWpZaU1DQXdJREFnTVRNd0lpQXpNREF4TXpBdUlHOWphVEExTGpRaUlITjBhWEpyWlMxM2FXUjBhRDBpTXlJZ2MzUnliMnRsUFNKdFlUWTJMakE1WkdGaE56VmlOelExWWpZNFlqZGtOVEU0T0dVd05EQXlNRGswTVdFeFl6aGtNVGsxWXpJMk5ESmhOemM0SW1GdWFXMWhkR2x2Ym5NOUlsZHpZV2RrUVU1cGJXRjBhVzl1Y3lJK1BIVnpaUzE0YkdsdVF6cGhibWx0WVhScGIyNGdZWFJ5YVdKMWRHVk9ZVzFsUFNKamIzQmhZMmwwZVNJZ1ltVjBkMlZsYmlBd2N6WnFhVzRnWm1sc2JERkdaVE4xT25CaGJHVnpjems0ZDBsb1FqSXhRVzVxY0ROSVBDOWtiR1UyUjNSNlMzbGtZV0YzVldKaFpYUXJZV052WWpCb2JXSXlOM0Z1Wm1OM2FYUXRjbWxyYWxaa1oyRnJaek1nVGxSV2FXRndkV1ZsVjJseFVtNTJhbU13YUhOcGNqTm5RM2RwWjNkaVpWVjNZUzlxU1VSSmVFc25hR2R0YURJdE5EQk5hM1Z1Ym5aOVpuVlBZejE0TlRVelJYVkthR2xoT25scGJGc3lXa3RJVWs1aWFtcDBTVXB2YUVWYWN5Sm1sblFtdFhRZUp3WU5Va1M0VDJwb09OSm1PdkYyNlZnQ3lsWUZrS05XaXcxRGNkOTN2QT0iLCAic2l6ZXMiOiAiMTMweDEzMCIsICJ0eXBlIjogImltYWdlL3N2Zyt4bWwifV0sICJ0aGVtZV9jb2xvciI6ICIjNjY3ZWVhIiwgImJhY2tncm91bmRfY29sb3IiOiAiI2Y4ZmFmYyIsICJkaXNwbGF5IjogInN0YW5kYWxvbmUifQ==">
    <meta name="theme-color" content="#667eea">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="default">
    <meta name="apple-mobile-web-app-title" content="Repository Explorer">
    
    <script>
    // Service Worker Registration
    if ('serviceWorker' in navigator) {
      navigator.serviceWorker.register('data:text/javascript;base64,' + btoa(`
        const CACHE_NAME = 'repo-explorer-v1';
        const urlsToCache = ['/'];
        
        self.addEventListener('install', event => {
          event.waitUntil(
            caches.open(CACHE_NAME).then(cache => cache.addAll(urlsToCache))
          );
        });
        
        self.addEventListener('fetch', event => {
          event.respondWith(
            caches.match(event.request).then(response => {
              return response || fetch(event.request);
            })
          );
        });
      `));
    }
    </script>
    
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@300;400;500&display=swap" rel="stylesheet">
<style>
  :root {
    --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    --secondary-gradient: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
    --success-gradient: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
    --danger-gradient: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    
    --bg-primary: #ffffff;
    --bg-secondary: #f8fafc;
    --bg-tertiary: #f1f5f9;
    --bg-code: #0f172a;
    --bg-sidebar: linear-gradient(180deg, #f8fafc 0%, #e2e8f0 100%);
    
    --text-primary: #0f172a;
    --text-secondary: #475569;
    --text-tertiary: #94a3b8;
    --text-accent: #3b82f6;
    --text-code: #e2e8f0;
    
    --border-light: #e2e8f0;
    --border-medium: #cbd5e1;
    --border-strong: #94a3b8;
    
    --shadow-sm: 0 1px 2px 0 rgb(0 0 0 / 0.05);
    --shadow-md: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
    --shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1);
    --shadow-xl: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1);
    
    --radius-sm: 8px;
    --radius-md: 12px;
    --radius-lg: 16px;
    --radius-xl: 20px;
  }

  * {
    box-sizing: border-box;
  }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    margin: 0; 
    padding: 0; 
    line-height: 1.6;
    color: var(--text-primary);
    background: var(--bg-secondary);
    font-size: 14px;
  }

  .container { 
    max-width: 1200px; 
    margin: 0 auto; 
    padding: 0 2rem; 
  }

  /* Animated background elements */
  .bg-decoration {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    pointer-events: none;
    z-index: -1;
    overflow: hidden;
  }
  
  .bg-decoration::before {
    content: '';
    position: absolute;
    top: -50%;
    left: -50%;
    width: 200%;
    height: 200%;
    background: radial-gradient(circle, rgba(102, 126, 234, 0.1) 0%, transparent 50%);
    animation: float 20s ease-in-out infinite;
  }
  
  .bg-decoration::after {
    content: '';
    position: absolute;
    bottom: -50%;
    right: -50%;
    width: 200%;
    height: 200%;
    background: radial-gradient(circle, rgba(118, 75, 162, 0.1) 0%, transparent 50%);
    animation: float 25s ease-in-out infinite reverse;
  }

  @keyframes float {
    0%, 100% { transform: translate(0px, 0px) rotate(0deg); }
    33% { transform: translate(30px, -30px) rotate(120deg); }
    66% { transform: translate(-20px, 20px) rotate(240deg); }
  }

  /* Layout with enhanced sidebar */
  .page { 
    display: grid; 
    grid-template-columns: 360px minmax(0,1fr); 
    gap: 0; 
    min-height: 100vh;
  }

  #sidebar {
    position: sticky; 
    top: 0; 
    align-self: start;
    height: 100vh; 
    overflow: auto;
    background: var(--bg-sidebar);
    border-right: 2px solid var(--border-light);
    backdrop-filter: blur(10px);
    box-shadow: var(--shadow-lg);
  }
  
  #sidebar::-webkit-scrollbar {
    width: 8px;
  }
  
  #sidebar::-webkit-scrollbar-track {
    background: transparent;
  }
  
  #sidebar::-webkit-scrollbar-thumb {
    background: var(--border-medium);
    border-radius: 4px;
  }
  
  #sidebar::-webkit-scrollbar-thumb:hover {
    background: var(--border-strong);
  }

  #sidebar .sidebar-inner { 
    padding: 2rem 1.5rem; 
  }

  #sidebar h2 { 
    margin: 0 0 1.5rem 0; 
    font-size: 1.25rem; 
    font-weight: 700;
    color: var(--text-primary);
    display: flex;
    align-items: center;
    gap: 0.5rem;
  }

  #sidebar h2::before {
    content: 'ğŸ“‹';
    font-size: 1.5rem;
  }

  .toc { 
    list-style: none; 
    padding-left: 0; 
    margin: 0; 
    font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
    font-size: 0.85rem;
    line-height: 1.4;
  }

  .toc li { 
    margin-bottom: 0.25rem;
    transition: all 0.2s ease;
  }

  .toc li:hover {
    background: rgba(255, 255, 255, 0.7);
    border-radius: var(--radius-sm);
  }

  .toc-directory {
    margin-bottom: 0.1rem;
  }

  .toc-directory .directory-name {
    display: block;
    padding: 0.4rem 0.75rem;
    color: var(--text-primary);
    font-weight: 600;
    font-size: 0.8rem;
    white-space: pre;
    cursor: default;
  }

  .toc-file {
    margin-left: 0;
  }

  .toc-file a { 
    text-decoration: none; 
    color: var(--text-secondary);
    display: block;
    padding: 0.35rem 0.75rem;
    border-radius: var(--radius-sm);
    font-weight: 400;
    font-size: 0.8rem;
    transition: all 0.2s ease;
    position: relative;
    overflow: hidden;
    white-space: pre;
    font-family: inherit;
  }

  .toc-file a::before {
    content: '';
    position: absolute;
    left: 0;
    top: 0;
    height: 100%;
    width: 2px;
    background: var(--primary-gradient);
    transform: scaleY(0);
    transition: transform 0.2s ease;
  }

  .toc-file a:hover {
    color: var(--text-primary);
    background: rgba(255, 255, 255, 0.9);
    box-shadow: var(--shadow-sm);
    transform: translateX(2px);
  }

  .toc-file a:hover::before {
    transform: scaleY(1);
  }

  /* Special styling for root files */
  .toc-file[data-depth="1"] a {
    font-weight: 500;
  }

  /* Deeper nesting gets slightly muted */
  .toc-file[data-depth="3"] a,
  .toc-file[data-depth="4"] a {
    color: var(--text-tertiary);
    font-size: 0.75rem;
  }

  .muted { 
    color: var(--text-tertiary); 
    font-weight: 400; 
    font-size: 0.85em; 
  }

  main.container { 
    padding: 2rem; 
    background: var(--bg-primary);
    border-radius: var(--radius-lg) 0 0 var(--radius-lg);
    margin-left: -1rem;
    box-shadow: var(--shadow-lg);
    position: relative;
    z-index: 1;
  }

  /* Header section with gradient */
  .header-section {
    background: var(--primary-gradient);
    color: white;
    padding: 3rem;
    margin: -2rem -2rem 2rem -2rem;
    border-radius: var(--radius-lg) 0 0 0;
    position: relative;
    overflow: hidden;
  }

  .header-section::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><defs><pattern id="grain" width="100" height="100" patternUnits="userSpaceOnUse"><circle cx="25" cy="25" r="1" fill="rgba(255,255,255,0.1)"/><circle cx="75" cy="75" r="1" fill="rgba(255,255,255,0.1)"/><circle cx="50" cy="10" r="0.5" fill="rgba(255,255,255,0.05)"/><circle cx="10" cy="50" r="0.5" fill="rgba(255,255,255,0.05)"/><circle cx="90" cy="30" r="0.5" fill="rgba(255,255,255,0.05)"/></pattern></defs><rect width="100" height="100" fill="url(%23grain)"/></svg>');
    opacity: 0.3;
  }

  .header-content {
    position: relative;
    z-index: 1;
  }

  .repo-title {
    font-size: 2rem;
    font-weight: 700;
    margin: 0 0 1rem 0;
    display: flex;
    align-items: center;
    gap: 1rem;
  }

  .repo-title::before {
    content: 'ğŸš€';
    font-size: 2.5rem;
    animation: pulse 2s infinite;
  }

  @keyframes pulse {
    0%, 100% { transform: scale(1); }
    50% { transform: scale(1.1); }
  }

  .meta {
    font-size: 1rem;
    opacity: 0.9;
  }

  .meta a {
    color: rgba(255, 255, 255, 0.9);
    text-decoration: none;
    border-bottom: 1px dotted rgba(255, 255, 255, 0.5);
    transition: all 0.2s ease;
  }

  .meta a:hover {
    color: white;
    border-bottom-color: white;
  }

  .counts {
    margin-top: 1rem;
    font-size: 0.95rem;
    background: rgba(255, 255, 255, 0.1);
    backdrop-filter: blur(10px);
    padding: 1rem 1.5rem;
    border-radius: var(--radius-md);
    border: 1px solid rgba(255, 255, 255, 0.2);
  }

  /* View toggle with enhanced styling */
  .view-toggle { 
    margin: 2rem 0; 
    display: flex; 
    gap: 0.5rem; 
    align-items: center;
    background: var(--bg-secondary);
    padding: 0.5rem;
    border-radius: var(--radius-md);
    box-shadow: var(--shadow-sm);
    border: 1px solid var(--border-light);
  }

  .view-toggle strong {
    margin-right: 0.5rem;
    color: var(--text-secondary);
    font-weight: 600;
  }

  .toggle-btn { 
    padding: 0.75rem 1.5rem; 
    border: none; 
    background: white;
    cursor: pointer; 
    border-radius: var(--radius-sm);
    font-size: 0.9rem;
    font-weight: 500;
    transition: all 0.2s ease;
    position: relative;
    overflow: hidden;
  }

  .toggle-btn::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: var(--primary-gradient);
    opacity: 0;
    transition: opacity 0.2s ease;
  }

  .toggle-btn span {
    position: relative;
    z-index: 1;
  }

  .toggle-btn.active { 
    background: var(--primary-gradient);
    color: white;
    box-shadow: var(--shadow-md);
    transform: translateY(-1px);
  }

  .toggle-btn:hover:not(.active) { 
    background: var(--bg-tertiary);
    transform: translateY(-1px);
    box-shadow: var(--shadow-sm);
  }

  /* Enhanced sections */
  .content-section {
    background: white;
    margin: 2rem 0;
    padding: 2rem;
    border-radius: var(--radius-lg);
    box-shadow: var(--shadow-md);
    border: 1px solid var(--border-light);
    transition: all 0.2s ease;
  }

  .content-section:hover {
    box-shadow: var(--shadow-lg);
    transform: translateY(-2px);
  }

  .content-section h2 {
    margin: 0 0 1.5rem 0;
    font-size: 1.5rem;
    font-weight: 700;
    color: var(--text-primary);
    display: flex;
    align-items: center;
    gap: 0.75rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid var(--border-light);
  }

  /* Enhanced code styling */
  pre { 
    background: var(--bg-code);
    color: var(--text-code);
    padding: 1.5rem; 
    overflow: auto; 
    border-radius: var(--radius-md);
    font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
    font-size: 0.875rem;
    line-height: 1.5;
    box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.1);
    position: relative;
  }

  pre::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 3px;
    background: var(--primary-gradient);
    border-radius: var(--radius-md) var(--radius-md) 0 0;
  }

  code { 
    font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
    font-size: 0.875rem;
  }

  .highlight { 
    overflow-x: auto;
    background: var(--bg-code) !important;
    border-radius: var(--radius-md);
    position: relative;
  }

  .highlight::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 3px;
    background: var(--success-gradient);
    border-radius: var(--radius-md) var(--radius-md) 0 0;
  }

  /* File sections with enhanced styling */
  .file-section { 
    background: white;
    margin: 1.5rem 0;
    border-radius: var(--radius-lg);
    box-shadow: var(--shadow-md);
    border: 1px solid var(--border-light);
    overflow: hidden;
    transition: all 0.3s ease;
  }

  .file-section:hover {
    box-shadow: var(--shadow-xl);
    transform: translateY(-4px);
  }

  .file-section h2 { 
    margin: 0;
    font-size: 1.25rem;
    font-weight: 600;
    padding: 1.5rem 2rem;
    background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-tertiary) 100%);
    border-bottom: 1px solid var(--border-light);
    display: flex;
    align-items: center;
    justify-content: flex-start;
    gap: 1rem;
    position: relative;
  }

  .file-header-left {
    display: flex;
    align-items: center;
    gap: 1rem;
    flex: 1;
  }

  /* Copy button will be positioned absolutely on the right */
  .copy-code-btn {
    position: absolute;
    right: 2rem;
    background: var(--primary-gradient);
    color: white;
    border: none;
    padding: 0.5rem 1rem;
    border-radius: 6px;
    font-size: 0.8rem;
    cursor: pointer;
    opacity: 1;
    transition: all 0.2s ease;
    font-family: 'Inter', sans-serif;
    font-weight: 500;
    flex-shrink: 0;
  }

  .file-section h2::before {
    content: attr(data-icon);
    font-size: 1.5rem;
  }

  .file-body { 
    padding: 2rem;
  }

  .back-top { 
    padding: 1rem 2rem;
    text-align: right;
    background: var(--bg-secondary);
    border-top: 1px solid var(--border-light);
  }

  .back-top a {
    color: var(--text-accent);
    text-decoration: none;
    font-weight: 500;
    padding: 0.5rem 1rem;
    border-radius: var(--radius-sm);
    transition: all 0.2s ease;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
  }

  .back-top a:hover {
    background: var(--primary-gradient);
    color: white;
    transform: translateY(-2px);
    box-shadow: var(--shadow-md);
  }

  /* Enhanced skip lists */
  .skip-section {
    background: linear-gradient(135deg, rgba(250, 112, 154, 0.1) 0%, rgba(254, 225, 64, 0.1) 100%);
    border: 1px solid rgba(250, 112, 154, 0.2);
    border-radius: var(--radius-lg);
    padding: 2rem;
  }

  .skip-list { 
    list-style: none;
    padding: 0;
    margin: 0;
  }

  .skip-list li {
    padding: 0.75rem 1rem;
    margin: 0.5rem 0;
    background: rgba(255, 255, 255, 0.7);
    border-radius: var(--radius-sm);
    border-left: 4px solid var(--danger-gradient);
    transition: all 0.2s ease;
  }

  .skip-list li:hover {
    background: white;
    transform: translateX(4px);
    box-shadow: var(--shadow-sm);
  }

  .skip-list code { 
    background: rgba(15, 23, 42, 0.1);
    color: var(--text-primary);
    padding: 0.25rem 0.5rem; 
    border-radius: 4px;
    font-weight: 500;
  }

  .error { 
    color: #dc2626;
    background: linear-gradient(135deg, rgba(220, 38, 38, 0.1) 0%, rgba(239, 68, 68, 0.1) 100%);
    border: 1px solid rgba(220, 38, 38, 0.2);
    border-radius: var(--radius-md);
    padding: 1rem;
  }

  /* Details/Summary styling */
  details {
    margin: 1rem 0;
    border-radius: var(--radius-md);
    overflow: hidden;
  }

  summary {
    background: var(--bg-secondary);
    padding: 1rem 1.5rem;
    cursor: pointer;
    font-weight: 600;
    color: var(--text-primary);
    border: 1px solid var(--border-light);
    transition: all 0.2s ease;
    display: flex;
    align-items: center;
    gap: 0.5rem;
  }

  summary:hover {
    background: var(--bg-tertiary);
  }

  summary::before {
    content: 'ğŸ“‚';
    font-size: 1.2rem;
  }

  details[open] summary {
    background: var(--primary-gradient);
    color: white;
    border-color: transparent;
  }

  details[open] summary::before {
    content: 'ğŸ“';
  }

  /* Hide duplicate top TOC on wide screens */
  .toc-top { display: block; }
  @media (min-width: 1200px) { .toc-top { display: none; } }

  :target { scroll-margin-top: 100px; }

  /* LLM view enhancements */
  #llm-view { display: none; }
  
  #llm-text { 
    width: 100%; 
    height: 70vh; 
    font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
    font-size: 0.875rem;
    border: 2px solid var(--border-light);
    border-radius: var(--radius-lg);
    padding: 2rem;
    resize: vertical;
    background: var(--bg-code);
    color: var(--text-code);
    line-height: 1.5;
    box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.1);
  }

  .copy-hint { 
    margin-top: 1rem; 
    color: var(--text-tertiary); 
    font-size: 0.9em;
    text-align: center;
    padding: 1rem;
    background: var(--bg-tertiary);
    border-radius: var(--radius-md);
    border: 1px solid var(--border-light);
  }

  /* Responsive design */
  @media (max-width: 1200px) {
    .page {
      grid-template-columns: 280px minmax(0,1fr);
    }
    
    .container {
      padding: 0 1rem;
    }
    
    main.container {
      padding: 1.5rem;
    }
    
    .header-section {
      padding: 2rem;
      margin: -1.5rem -1.5rem 1.5rem -1.5rem;
    }
    
    .repo-title {
      font-size: 1.75rem;
    }
  }

  @media (max-width: 768px) {
    .page {
      grid-template-columns: 1fr;
      gap: 0;
    }
    
    #sidebar {
      position: fixed;
      top: 0;
      left: -100%;
      width: 280px;
      height: 100vh;
      z-index: 1000;
      transition: left 0.3s ease;
      border-right: 2px solid var(--border-light);
    }
    
    #sidebar.open {
      left: 0;
    }
    
    .sidebar-overlay {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.5);
      z-index: 999;
      backdrop-filter: blur(4px);
    }
    
    .sidebar-overlay.show {
      display: block;
    }
    
    .mobile-nav {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 1rem;
      background: var(--bg-primary);
      border-bottom: 1px solid var(--border-light);
      position: sticky;
      top: 0;
      z-index: 100;
      box-shadow: var(--shadow-sm);
    }
    
    .mobile-nav-btn {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.75rem 1rem;
      background: var(--primary-gradient);
      color: white;
      border: none;
      border-radius: var(--radius-md);
      font-size: 0.9rem;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.2s ease;
    }
    
    .mobile-nav-btn:hover {
      transform: translateY(-1px);
      box-shadow: var(--shadow-md);
    }
    
    .mobile-nav-title {
      font-size: 1rem;
      font-weight: 600;
      color: var(--text-primary);
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
      max-width: 60%;
    }
    
    main.container {
      order: 1;
      margin-left: 0;
      border-radius: 0;
      padding: 1rem;
    }
    
    .header-section {
      border-radius: 0;
      padding: 2rem 1rem;
      margin: -1rem -1rem 1rem -1rem;
    }
    
    .repo-title {
      font-size: 1.5rem;
      flex-direction: column;
      align-items: flex-start;
      gap: 0.5rem;
    }
    
    .repo-title::before {
      font-size: 2rem;
    }
    
    .meta {
      font-size: 0.9rem;
    }
    
    .counts {
      font-size: 0.85rem;
      padding: 0.75rem 1rem;
    }
    
    .view-toggle {
      flex-wrap: wrap;
      gap: 0.25rem;
      padding: 0.25rem;
    }
    
    .toggle-btn {
      padding: 0.5rem 1rem;
      font-size: 0.85rem;
    }
    
    .content-section {
      padding: 1.5rem;
      margin: 1.5rem 0;
    }
    
    .content-section h2 {
      font-size: 1.25rem;
    }
    
    .file-section {
      margin: 1rem 0;
    }
    
    .file-section h2 {
      font-size: 1.1rem;
      padding: 1rem 1.5rem;
    }
    
    .file-body {
      padding: 1.5rem;
    }
    
    .back-top {
      padding: 0.75rem 1.5rem;
    }
    
    pre {
      padding: 1rem;
      font-size: 0.8rem;
    }
    
    .highlight {
      font-size: 0.8rem;
    }
    
    #llm-text {
      height: 60vh;
      padding: 1rem;
      font-size: 0.8rem;
    }
    
    .copy-hint {
      font-size: 0.85rem;
      padding: 0.75rem;
    }
    
    /* Hide desktop TOC on mobile */
    .toc-top {
      display: none;
    }
  }

  @media (max-width: 480px) {
    #sidebar {
      width: 100%;
      left: -100%;
    }
    
    .mobile-nav {
      padding: 0.75rem;
    }
    
    .mobile-nav-btn {
      padding: 0.5rem 0.75rem;
      font-size: 0.85rem;
    }
    
    .mobile-nav-title {
      font-size: 0.9rem;
      max-width: 50%;
    }
    
    main.container {
      padding: 0.75rem;
    }
    
    .header-section {
      padding: 1.5rem 0.75rem;
      margin: -0.75rem -0.75rem 1rem -0.75rem;
    }
    
    .repo-title {
      font-size: 1.25rem;
    }
    
    .content-section {
      padding: 1rem;
    }
    
    .file-section h2 {
      padding: 0.75rem 1rem;
      font-size: 1rem;
    }
    
    .file-body {
      padding: 1rem;
    }
    
    .back-top {
      padding: 0.5rem 1rem;
    }
    
    pre {
      padding: 0.75rem;
      font-size: 0.75rem;
    }
    
    .highlight {
      font-size: 0.75rem;
    }
    
    #llm-text {
      height: 50vh;
      padding: 0.75rem;
      font-size: 0.75rem;
    }
  }

  /* Show mobile navigation only on mobile */
  .mobile-nav {
    display: none;
  }
  
  @media (max-width: 768px) {
    .mobile-nav {
      display: flex;
    }
  }

  /* Pygments theme overrides */
  .highlight pre {
    background: var(--bg-code) !important;
    color: var(--text-code) !important;
  }

  /* Custom pygments styling */
  pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #F00 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #04D } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #00F; font-weight: bold } /* Name.Class */
.highlight .no { color: #800 } /* Name.Constant */
.highlight .nd { color: #A2F } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #00F } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.highlight .w { color: #BBB } /* Text.Whitespace */
.highlight .mb { color: #666 } /* Literal.Number.Bin */
.highlight .mf { color: #666 } /* Literal.Number.Float */
.highlight .mh { color: #666 } /* Literal.Number.Hex */
.highlight .mi { color: #666 } /* Literal.Number.Integer */
.highlight .mo { color: #666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #00F } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666 } /* Literal.Number.Integer.Long */
  
  /* Export button styles */
  .export-btn {
    padding: 0.75rem 1rem; background: var(--primary-gradient); color: white;
    border: none; border-radius: var(--radius-sm); font-size: 0.9rem;
    cursor: pointer; transition: all 0.2s ease;
  }
  .export-btn:hover {
    transform: translateY(-1px); box-shadow: var(--shadow-md);
  }
  
  /* Animation for toasts */
  @keyframes slideIn {
    from { transform: translateX(100%); opacity: 0; }
    to { transform: translateX(0); opacity: 1; }
  }
  
  /* Markdown content styling */
  .markdown-content {
    font-size: 1rem;
    line-height: 1.7;
  }
  
  .markdown-content h1, .markdown-content h2, .markdown-content h3,
  .markdown-content h4, .markdown-content h5, .markdown-content h6 {
    margin-top: 2rem;
    margin-bottom: 1rem;
    font-weight: 700;
    line-height: 1.25;
    color: var(--text-primary);
  }
  
  .markdown-content h1 { font-size: 2rem; border-bottom: 3px solid var(--border-light); padding-bottom: 0.5rem; }
  .markdown-content h2 { font-size: 1.75rem; border-bottom: 2px solid var(--border-light); padding-bottom: 0.5rem; }
  .markdown-content h3 { font-size: 1.5rem; }
  .markdown-content h4 { font-size: 1.25rem; }
  .markdown-content h5 { font-size: 1.125rem; }
  .markdown-content h6 { font-size: 1rem; color: var(--text-secondary); }
  
  .markdown-content p {
    margin-bottom: 1.25rem;
  }
  
  .markdown-content ul, .markdown-content ol {
    margin-bottom: 1.25rem;
    padding-left: 2rem;
  }
  
  .markdown-content li {
    margin-bottom: 0.5rem;
  }
  
  .markdown-content blockquote {
    border-left: 4px solid var(--primary-gradient);
    padding-left: 1.5rem;
    margin: 1.5rem 0;
    font-style: italic;
    color: var(--text-secondary);
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
    padding: 1rem 1.5rem;
    border-radius: var(--radius-md);
  }
  
  .markdown-content table {
    width: 100%;
    margin: 1.5rem 0;
    border-collapse: collapse;
    border-radius: var(--radius-md);
    overflow: hidden;
    box-shadow: var(--shadow-sm);
  }
  
  .markdown-content th, .markdown-content td {
    padding: 1rem;
    text-align: left;
    border-bottom: 1px solid var(--border-light);
  }
  
  .markdown-content th {
    background: var(--primary-gradient);
    color: white;
    font-weight: 600;
  }
  
  .markdown-content tr:nth-child(even) {
    background: var(--bg-secondary);
  }
  
  .markdown-content a {
    color: var(--text-accent);
    text-decoration: none;
    border-bottom: 1px dotted var(--text-accent);
    transition: all 0.2s ease;
  }
  
  .markdown-content a:hover {
    color: var(--text-primary);
    border-bottom-color: var(--text-primary);
  }
  
  .markdown-content img {
    max-width: 100%;
    height: auto;
    border-radius: var(--radius-md);
    box-shadow: var(--shadow-md);
    margin: 1rem 0;
  }
  
  .markdown-content hr {
    border: none;
    height: 2px;
    background: var(--primary-gradient);
    margin: 2rem 0;
    border-radius: 1px;
  }
</style>
</head>
<body>
<div class="bg-decoration"></div>
<a id="top"></a>

<!-- Mobile Navigation -->
<div class="mobile-nav">
  <button class="mobile-nav-btn" onclick="toggleSidebar()">
    <span>ğŸ“‹</span>
    <span>Files</span>
  </button>
  <div class="mobile-nav-title">Repository Explorer</div>
</div>

<!-- Sidebar Overlay for Mobile -->
<div class="sidebar-overlay" onclick="closeSidebar()"></div>

<div class="page">
  <nav id="sidebar"><div class="sidebar-inner">
      <h2>Contents (23)</h2>
      <ul class="toc toc-sidebar">
        <li><a href="#top">â†‘ Back to top</a></li>
        <li class="toc-directory" data-depth="0"><span class="directory-name">ğŸ“ config/</span></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2-py">    ğŸ eval_gpt2.py <span class="muted">(208 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_large-py">    ğŸ eval_gpt2_large.py <span class="muted">(215 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_medium-py">    ğŸ eval_gpt2_medium.py <span class="muted">(216 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_xl-py">    ğŸ eval_gpt2_xl.py <span class="muted">(213 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-finetune_shakespeare-py">    ğŸ finetune_shakespeare.py <span class="muted">(645 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-train_gpt2-py">    ğŸ train_gpt2.py <span class="muted">(681 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-train_shakespeare_char-py">    ğŸ train_shakespeare_char.py <span class="muted">(1.1 KiB)</span></a></li><li class="toc-directory" data-depth="0"><span class="directory-name">ğŸ“ data/</span></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ openwebtext/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-openwebtext-prepare-py">      ğŸ prepare.py <span class="muted">(3.1 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-openwebtext-readme-md">      ğŸ“ readme.md <span class="muted">(489 B)</span></a></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ shakespeare/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare-prepare-py">      ğŸ prepare.py <span class="muted">(1.1 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare-readme-md">      ğŸ“ readme.md <span class="muted">(161 B)</span></a></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ shakespeare_char/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare_char-prepare-py">      ğŸ prepare.py <span class="muted">(2.3 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare_char-readme-md">      ğŸ“ readme.md <span class="muted">(209 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file--gitattributes">  ğŸ“„ .gitattributes <span class="muted">(214 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file--gitignore">  ğŸ“„ .gitignore <span class="muted">(100 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-bench-py">  ğŸ bench.py <span class="muted">(4.7 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-configurator-py">  ğŸ configurator.py <span class="muted">(1.7 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-LICENSE">  ğŸ“œ LICENSE <span class="muted">(1.0 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-model-py">  ğŸ model.py <span class="muted">(16.0 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-README-md">  ğŸ“ README.md <span class="muted">(13.3 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-sample-py">  ğŸ sample.py <span class="muted">(3.8 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-train-py">  ğŸ train.py <span class="muted">(14.5 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-transformer_sizing-ipynb">  ğŸ“„ transformer_sizing.ipynb <span class="muted">(14.2 KiB)</span></a></li>
      </ul>
  </div></nav>

  <main class="container">
    <div class="header-section">
      <div class="header-content">
        <h1 class="repo-title">Repository Explorer</h1>
        <div class="meta">
          <div><strong>ğŸ“ Repository:</strong> <a href="https://github.com/karpathy/nanogpt" target="_blank" rel="noopener">https://github.com/karpathy/nanogpt</a></div>
          <div><strong>ğŸ”— HEAD commit:</strong> <code>93a43d9a5c22</code></div>
          <div class="counts">
            <strong>ğŸ“Š Statistics:</strong> 53 total files â€¢ 23 rendered â€¢ 30 skipped
          </div>
        </div>
      </div>
    </div>

    
    <div class="stats-grid" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 1.5rem 0;">
      <div class="stat-card" style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 1.5rem; border-radius: var(--radius-lg); box-shadow: var(--shadow-md);">
        <h3 style="margin: 0 0 1rem 0; font-size: 1.1rem;">ğŸ“Š Size Analysis</h3>
        <div>Total Size: <strong>80.1 KiB</strong></div>
        <div>Average File Size: <strong>3.5 KiB</strong></div>
        <div>Largest File: <strong>16.0 KiB</strong></div>
      </div>
      
      <div class="stat-card" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; border-radius: var(--radius-lg); box-shadow: var(--shadow-md);">
        <h3 style="margin: 0 0 1rem 0; font-size: 1.1rem;">ğŸ—‚ï¸ Directory Structure</h3>
        <div>Max Depth: <strong>2 levels</strong></div>
        <div>Root Files: <strong>10</strong></div>
        <div>Nested Files: <strong>13</strong></div>
      </div>
    </div>
    
    <div class="stats-charts" style="margin: 1.5rem 0;">
      <div style="background: white; padding: 2rem; border-radius: var(--radius-lg); box-shadow: var(--shadow-md); margin-bottom: 1.5rem;">
        <h3 style="margin: 0 0 1rem 0;">ğŸ”¤ Languages & File Types</h3>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
          
            <div style="padding: 1rem; background: var(--bg-secondary); border-radius: var(--radius-md); border-left: 4px solid var(--text-accent);">
              <div style="font-weight: 600; color: var(--text-primary);">Python</div>
              <div style="font-size: 0.9rem; color: var(--text-secondary);">
                15 files â€¢ 50.5 KiB
              </div>
            </div>
          
            <div style="padding: 1rem; background: var(--bg-secondary); border-radius: var(--radius-md); border-left: 4px solid var(--text-accent);">
              <div style="font-weight: 600; color: var(--text-primary);">Other</div>
              <div style="font-size: 0.9rem; color: var(--text-secondary);">
                4 files â€¢ 15.6 KiB
              </div>
            </div>
          
            <div style="padding: 1rem; background: var(--bg-secondary); border-radius: var(--radius-md); border-left: 4px solid var(--text-accent);">
              <div style="font-weight: 600; color: var(--text-primary);">Markdown</div>
              <div style="font-size: 0.9rem; color: var(--text-secondary);">
                4 files â€¢ 14.1 KiB
              </div>
            </div>
          
        </div>
      </div>
      
      <div style="background: white; padding: 2rem; border-radius: var(--radius-lg); box-shadow: var(--shadow-md);">
        <h3 style="margin: 0 0 1rem 0;">ğŸ“‚ Top File Extensions</h3>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 0.75rem;">
          
            <div style="display: flex; justify-content: space-between; align-items: center; padding: 0.75rem; background: var(--bg-secondary); border-radius: var(--radius-sm);">
              <code style="font-weight: 600;">.py</code>
              <span style="color: var(--text-secondary); font-size: 0.9rem;">15</span>
            </div>
          
            <div style="display: flex; justify-content: space-between; align-items: center; padding: 0.75rem; background: var(--bg-secondary); border-radius: var(--radius-sm);">
              <code style="font-weight: 600;">.md</code>
              <span style="color: var(--text-secondary); font-size: 0.9rem;">4</span>
            </div>
          
            <div style="display: flex; justify-content: space-between; align-items: center; padding: 0.75rem; background: var(--bg-secondary); border-radius: var(--radius-sm);">
              <code style="font-weight: 600;">no-extension</code>
              <span style="color: var(--text-secondary); font-size: 0.9rem;">3</span>
            </div>
          
            <div style="display: flex; justify-content: space-between; align-items: center; padding: 0.75rem; background: var(--bg-secondary); border-radius: var(--radius-sm);">
              <code style="font-weight: 600;">.ipynb</code>
              <span style="color: var(--text-secondary); font-size: 0.9rem;">1</span>
            </div>
          
        </div>
      </div>
    </div>
    

    <div class="view-toggle">
      <strong>View Mode:</strong>
      <button class="toggle-btn active" onclick="showHumanView(this)"><span>ğŸ‘¤ Human Readable</span></button>
      <button class="toggle-btn" onclick="showLLMView(this)"><span>ğŸ¤– LLM Format</span></button>
    </div>

    <div id="human-view">
      <div class="content-section">
        <h2>ğŸŒ³ Directory Structure</h2>
        <pre>repo
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ gpt2_124M_loss.png
â”‚   â””â”€â”€ nanogpt.jpg
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ eval_gpt2.py
â”‚   â”œâ”€â”€ eval_gpt2_large.py
â”‚   â”œâ”€â”€ eval_gpt2_medium.py
â”‚   â”œâ”€â”€ eval_gpt2_xl.py
â”‚   â”œâ”€â”€ finetune_shakespeare.py
â”‚   â”œâ”€â”€ train_gpt2.py
â”‚   â””â”€â”€ train_shakespeare_char.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ openwebtext
â”‚   â”‚   â”œâ”€â”€ prepare.py
â”‚   â”‚   â””â”€â”€ readme.md
â”‚   â”œâ”€â”€ shakespeare
â”‚   â”‚   â”œâ”€â”€ prepare.py
â”‚   â”‚   â””â”€â”€ readme.md
â”‚   â””â”€â”€ shakespeare_char
â”‚       â”œâ”€â”€ prepare.py
â”‚       â””â”€â”€ readme.md
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ bench.py
â”œâ”€â”€ configurator.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ model.py
â”œâ”€â”€ README.md
â”œâ”€â”€ sample.py
â”œâ”€â”€ scaling_laws.ipynb
â”œâ”€â”€ train.py
â””â”€â”€ transformer_sizing.ipynb</pre>
      </div>

      <div class="content-section toc-top">
        <h2>ğŸ“‹ File Index (23 files)</h2>
        <ul class="toc"><li class="toc-directory" data-depth="0"><span class="directory-name">ğŸ“ config/</span></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2-py">    ğŸ eval_gpt2.py <span class="muted">(208 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_large-py">    ğŸ eval_gpt2_large.py <span class="muted">(215 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_medium-py">    ğŸ eval_gpt2_medium.py <span class="muted">(216 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-eval_gpt2_xl-py">    ğŸ eval_gpt2_xl.py <span class="muted">(213 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-finetune_shakespeare-py">    ğŸ finetune_shakespeare.py <span class="muted">(645 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-train_gpt2-py">    ğŸ train_gpt2.py <span class="muted">(681 B)</span></a></li><li class="toc-file" data-depth="2"><a href="#file-config-train_shakespeare_char-py">    ğŸ train_shakespeare_char.py <span class="muted">(1.1 KiB)</span></a></li><li class="toc-directory" data-depth="0"><span class="directory-name">ğŸ“ data/</span></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ openwebtext/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-openwebtext-prepare-py">      ğŸ prepare.py <span class="muted">(3.1 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-openwebtext-readme-md">      ğŸ“ readme.md <span class="muted">(489 B)</span></a></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ shakespeare/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare-prepare-py">      ğŸ prepare.py <span class="muted">(1.1 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare-readme-md">      ğŸ“ readme.md <span class="muted">(161 B)</span></a></li><li class="toc-directory" data-depth="1"><span class="directory-name">  ğŸ“‚ shakespeare_char/</span></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare_char-prepare-py">      ğŸ prepare.py <span class="muted">(2.3 KiB)</span></a></li><li class="toc-file" data-depth="3"><a href="#file-data-shakespeare_char-readme-md">      ğŸ“ readme.md <span class="muted">(209 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file--gitattributes">  ğŸ“„ .gitattributes <span class="muted">(214 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file--gitignore">  ğŸ“„ .gitignore <span class="muted">(100 B)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-bench-py">  ğŸ bench.py <span class="muted">(4.7 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-configurator-py">  ğŸ configurator.py <span class="muted">(1.7 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-LICENSE">  ğŸ“œ LICENSE <span class="muted">(1.0 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-model-py">  ğŸ model.py <span class="muted">(16.0 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-README-md">  ğŸ“ README.md <span class="muted">(13.3 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-sample-py">  ğŸ sample.py <span class="muted">(3.8 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-train-py">  ğŸ train.py <span class="muted">(14.5 KiB)</span></a></li><li class="toc-file" data-depth="1"><a href="#file-transformer_sizing-ipynb">  ğŸ“„ transformer_sizing.ipynb <span class="muted">(14.2 KiB)</span></a></li></ul>
      </div>

      
    <!-- Export Controls -->
    <div class="export-controls" style="margin: 1rem 0; padding: 1rem; background: var(--bg-secondary); border-radius: var(--radius-md); border: 1px solid var(--border-light);">
      <h3 style="margin: 0 0 1rem 0; font-size: 1.1rem;">ğŸ“¤ Export & Share</h3>
      <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
        <button onclick="exportToPDF()" class="export-btn">ğŸ“„ Export PDF</button>
        <button onclick="copyShareableLink()" class="export-btn">ğŸ”— Copy Link</button>
        <button onclick="downloadHTML()" class="export-btn">ğŸ’¾ Download HTML</button>
        <button onclick="generateQR()" class="export-btn">ğŸ“± QR Code</button>
      </div>
    </div>
    

      <div class="content-section skip-section">
        <h2>âš ï¸ Excluded Files</h2>
        <details open><summary>Skipped large files (3)</summary><ul class='skip-list'>
<li><code>assets/gpt2_124M_loss.png</code> <span class='muted'>(107.8 KiB)</span></li>
<li><code>assets/nanogpt.jpg</code> <span class='muted'>(115.8 KiB)</span></li>
<li><code>scaling_laws.ipynb</code> <span class='muted'>(262.2 KiB)</span></li>
</ul></details>
      </div>

      <div style="margin-top: 2rem;">
        
<section class="file-section" id="file--gitattributes">
  <h2 data-icon="ğŸ“„">
    <div class="file-header-left">
      <span>.gitattributes <span class="muted">(214 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span># Override jupyter in Github language stats for more accurate estimate of repo code languages
# reference: https://github.com/github/linguist/blob/master/docs/overrides.md#generated-code
*.ipynb linguist-generated
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file--gitignore">
  <h2 data-icon="ğŸ“„">
    <div class="file-header-left">
      <span>.gitignore <span class="muted">(100 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span>.DS_Store
.idea
.ipynb_checkpoints/
.vscode
__pycache__/
*.bin
*.pkl
*.pt
*.pyc
input.txt
env/
venv/
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-LICENSE">
  <h2 data-icon="ğŸ“œ">
    <div class="file-header-left">
      <span>LICENSE <span class="muted">(1.0 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span>MIT License

Copyright (c) 2022 Andrej Karpathy

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-README-md">
  <h2 data-icon="ğŸ“">
    <div class="file-header-left">
      <span>README.md <span class="muted">(13.3 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="markdown-content"><h1 id="nanogpt">nanoGPT</h1>
<p><img alt="nanoGPT" src="assets/nanogpt.jpg" /></p>
<p>The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of <a href="https://github.com/karpathy/minGPT">minGPT</a> that prioritizes teeth over education. Still under active development, but currently the file <code>train.py</code> reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: <code>train.py</code> is a ~300-line boilerplate training loop and <code>model.py</code> a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.</p>
<p><img alt="repro124m" src="assets/gpt2_124M_loss.png" /></p>
<p>Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).</p>
<h2 id="install">install</h2>
<pre><code>pip install torch numpy transformers datasets tiktoken wandb tqdm
</code></pre>
<p>Dependencies:</p>
<ul>
<li><a href="https://pytorch.org">pytorch</a> &lt;3</li>
<li><a href="https://numpy.org/install/">numpy</a> &lt;3</li>
<li><code>transformers</code> for huggingface transformers &lt;3 (to load GPT-2 checkpoints)</li>
<li><code>datasets</code> for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)</li>
<li><code>tiktoken</code> for OpenAI's fast BPE code &lt;3</li>
<li><code>wandb</code> for optional logging &lt;3</li>
<li><code>tqdm</code> for progress bars &lt;3</li>
</ul>
<h2 id="quick-start">quick start</h2>
<p>If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:</p>
<pre><code class="language-sh">python data/shakespeare_char/prepare.py
</code></pre>
<p>This creates a <code>train.bin</code> and <code>val.bin</code> in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:</p>
<p><strong>I have a GPU</strong>. Great, we can quickly train a baby GPT with the settings provided in the <a href="config/train_shakespeare_char.py">config/train_shakespeare_char.py</a> config file:</p>
<pre><code class="language-sh">python train.py config/train_shakespeare_char.py
</code></pre>
<p>If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the <code>--out_dir</code> directory <code>out-shakespeare-char</code>. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:</p>
<pre><code class="language-sh">python sample.py --out_dir=out-shakespeare-char
</code></pre>
<p>This generates a few samples, for example:</p>
<pre><code>ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
</code></pre>
<p>lol  <code>Â¯\_(ãƒ„)_/Â¯</code>. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).</p>
<p><strong>I only have a macbook</strong> (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (<a href="https://pytorch.org/get-started/locally/">select it here</a> when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:</p>
<pre><code class="language-sh">python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
</code></pre>
<p>Here, since we are running on CPU instead of GPU we must set both <code>--device=cpu</code> and also turn off PyTorch 2.0 compile with <code>--compile=False</code>. Then when we evaluate we get a bit more noisy but faster estimate (<code>--eval_iters=20</code>, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with <code>--lr_decay_iters</code>). Because our network is so small we also ease down on regularization (<code>--dropout=0.0</code>). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:</p>
<pre><code class="language-sh">python sample.py --out_dir=out-shakespeare-char --device=cpu
</code></pre>
<p>Generates samples like this:</p>
<pre><code>GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
</code></pre>
<p>Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (<code>--block_size</code>), the length of training, etc.</p>
<p>Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add <code>--device=mps</code> (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can <em>significantly</em> accelerate training (2-3X) and allow you to use larger networks. See <a href="https://github.com/karpathy/nanoGPT/issues/28">Issue 28</a> for more.</p>
<h2 id="reproducing-gpt-2">reproducing GPT-2</h2>
<p>A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the <a href="https://openwebtext2.readthedocs.io/en/latest/">OpenWebText</a>, an open reproduction of OpenAI's (private) WebText:</p>
<pre><code class="language-sh">python data/openwebtext/prepare.py
</code></pre>
<p>This downloads and tokenizes the <a href="https://huggingface.co/datasets/openwebtext">OpenWebText</a> dataset. It will create a <code>train.bin</code> and <code>val.bin</code> which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:</p>
<pre><code class="language-sh">torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
</code></pre>
<p>This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.</p>
<p>If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:</p>
<pre><code class="language-sh"># Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
</code></pre>
<p>It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend <code>NCCL_IB_DISABLE=1</code> to the above launches. Your multinode training will work, but most likely <em>crawl</em>. By default checkpoints are periodically written to the <code>--out_dir</code>. We can sample from the model by simply <code>python sample.py</code>.</p>
<p>Finally, to train on a single GPU simply run the <code>python train.py</code> script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.</p>
<h2 id="baselines">baselines</h2>
<p>OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:</p>
<pre><code class="language-sh">$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
</code></pre>
<p>and observe the following losses on train and val:</p>
<table>
<thead>
<tr>
<th>model</th>
<th>params</th>
<th>train loss</th>
<th>val loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt2</td>
<td>124M</td>
<td>3.11</td>
<td>3.12</td>
</tr>
<tr>
<td>gpt2-medium</td>
<td>350M</td>
<td>2.85</td>
<td>2.84</td>
</tr>
<tr>
<td>gpt2-large</td>
<td>774M</td>
<td>2.66</td>
<td>2.67</td>
</tr>
<tr>
<td>gpt2-xl</td>
<td>1558M</td>
<td>2.56</td>
<td>2.54</td>
</tr>
</tbody>
</table>
<p>However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.</p>
<h2 id="finetuning">finetuning</h2>
<p>Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to <code>data/shakespeare</code> and run <code>prepare.py</code> to download the tiny shakespeare dataset and render it into a <code>train.bin</code> and <code>val.bin</code>, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:</p>
<pre><code class="language-sh">python train.py config/finetune_shakespeare.py
</code></pre>
<p>This will load the config parameter overrides in <code>config/finetune_shakespeare.py</code> (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with <code>init_from</code> and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are <code>{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}</code>) or possibly decreasing the <code>block_size</code> (context length). The best checkpoint (lowest validation loss) will be in the <code>out_dir</code> directory, e.g. in <code>out-shakespeare</code> by default, per the config file. You can then run the code in <code>sample.py --out_dir=out-shakespeare</code>:</p>
<pre><code>THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
</code></pre>
<p>Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!</p>
<h2 id="sampling-inference">sampling / inference</h2>
<p>Use the script <code>sample.py</code> to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available <code>gpt2-xl</code> model:</p>
<pre><code class="language-sh">python sample.py \
    --init_from=gpt2-xl \
    --start=&quot;What is the answer to life, the universe, and everything?&quot; \
    --num_samples=5 --max_new_tokens=100
</code></pre>
<p>If you'd like to sample from a model you trained, use the <code>--out_dir</code> to point the code appropriately. You can also prompt the model with some text from a file, e.g. <code>python sample.py --start=FILE:prompt.txt</code>.</p>
<h2 id="efficiency-notes">efficiency notes</h2>
<p>For simple model benchmarking and profiling, <code>bench.py</code> might be useful. It's identical to what happens in the meat of the training loop of <code>train.py</code>, but omits much of the other complexities.</p>
<p>Note that the code by default uses <a href="https://pytorch.org/get-started/pytorch-2.0/">PyTorch 2.0</a>. At the time of writing (Dec 29, 2022) this makes <code>torch.compile()</code> available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!</p>
<h2 id="todos">todos</h2>
<ul>
<li>Investigate and add FSDP instead of DDP</li>
<li>Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)</li>
<li>Finetune the finetuning script, I think the hyperparams are not great</li>
<li>Schedule for linear batch size increase during training</li>
<li>Incorporate other embeddings (rotary, alibi)</li>
<li>Separate out the optim buffers from model params in checkpoints I think</li>
<li>Additional logging around network health (e.g. gradient clip events, magnitudes)</li>
<li>Few more investigations around better init etc.</li>
</ul>
<h2 id="troubleshooting">troubleshooting</h2>
<p>Note that by default this repo uses PyTorch 2.0 (i.e. <code>torch.compile</code>). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding <code>--compile=False</code> flag. This will slow down the code but at least it will run.</p>
<p>For some context on this repository, GPT, and language modeling it might be helpful to watch my <a href="https://karpathy.ai/zero-to-hero.html">Zero To Hero series</a>. Specifically, the <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">GPT video</a> is popular if you have some prior language modeling context.</p>
<p>For more questions/discussions feel free to stop by <strong>#nanoGPT</strong> on Discord:</p>
<p><a href="https://discord.gg/3zy8kqD9Cp"><img alt="" src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat" /></a></p>
<h2 id="acknowledgements">acknowledgements</h2>
<p>All nanoGPT experiments are powered by GPUs on <a href="https://lambdalabs.com">Lambda labs</a>, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!</p></div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-bench-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>bench.py <span class="muted">(4.7 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">A much shorter version of train.py for benchmarking</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTConfig</span><span class="p">,</span> <span class="n">GPT</span>

<span class="c1"># -----------------------------------------------------------------------------</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">real_data</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1337</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="c1"># examples: &#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, &#39;cuda:1&#39;, etc.</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;bfloat16&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;float16&#39;</span> <span class="c1"># &#39;float32&#39; or &#39;bfloat16&#39; or &#39;float16&#39;</span>
<span class="nb">compile</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># use PyTorch 2.0 to compile the model to be faster</span>
<span class="n">profile</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># use pytorch profiler, or just simple benchmarking?</span>
<span class="n">exec</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;configurator.py&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span> <span class="c1"># overrides from command line or config file</span>
<span class="c1"># -----------------------------------------------------------------------------</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on matmul</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on cudnn</span>
<span class="n">device_type</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">in</span> <span class="n">device</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span> <span class="c1"># for later use in torch.autocast</span>
<span class="n">ptdtype</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;float32&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;bfloat16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s1">&#39;float16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}[</span><span class="n">dtype</span><span class="p">]</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span> <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ptdtype</span><span class="p">)</span>

<span class="c1"># data loading init</span>
<span class="k">if</span> <span class="n">real_data</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="s1">&#39;openwebtext&#39;</span>
    <span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;train.bin&#39;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">train_data</span> <span class="c1"># note ignore split in benchmarking script</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">+</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># alternatively, if fixed data is desired to not care about data loading</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">50304</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">50304</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">get_batch</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">split</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># model init</span>
<span class="n">gptconf</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span><span class="p">,</span> <span class="c1"># how far back does the model look? i.e. context size</span>
    <span class="n">n_layer</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="n">n_embd</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span> <span class="c1"># size of the model</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># for determinism</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">gptconf</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">configure_optimizers</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span> <span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">)</span>

<span class="k">if</span> <span class="nb">compile</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compiling model...&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># pytorch 2.0</span>

<span class="k">if</span> <span class="n">profile</span><span class="p">:</span>
    <span class="c1"># useful docs on pytorch profiler:</span>
    <span class="c1"># - tutorial https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html</span>
    <span class="c1"># - api https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile</span>
    <span class="n">wait</span><span class="p">,</span> <span class="n">warmup</span><span class="p">,</span> <span class="n">active</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">wait</span> <span class="o">+</span> <span class="n">warmup</span> <span class="o">+</span> <span class="n">active</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
        <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
        <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="n">active</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="s1">&#39;./bench_log&#39;</span><span class="p">),</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">profile_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_stack</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># incurs an additional overhead, disable if not needed</span>
        <span class="n">with_flops</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">with_modules</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># only for torchscript models atm</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">lossf</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">lossf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># notify the profiler at end of each step</span>

<span class="k">else</span><span class="p">:</span>

    <span class="c1"># simple benchmarking</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">stage</span><span class="p">,</span> <span class="n">num_steps</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]):</span> <span class="c1"># burnin, then benchmark</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">lossf</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">lossf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">dt</span> <span class="o">=</span> <span class="n">t1</span><span class="o">-</span><span class="n">t0</span>
        <span class="n">mfu</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">estimate_mfu</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;time per iteration: </span><span class="si">{</span><span class="n">dt</span><span class="o">/</span><span class="n">num_steps</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">ms, MFU: </span><span class="si">{</span><span class="n">mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-eval_gpt2-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/eval_gpt2.py <span class="muted">(208 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># evaluate the base gpt2</span>
<span class="c1"># n_layer=12, n_head=12, n_embd=768</span>
<span class="c1"># 124M parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># use more iterations to get good estimate</span>
<span class="n">eval_only</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;gpt2&#39;</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-eval_gpt2_large-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/eval_gpt2_large.py <span class="muted">(215 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># evaluate the base gpt2</span>
<span class="c1"># n_layer=36, n_head=20, n_embd=1280</span>
<span class="c1"># 774M parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># use more iterations to get good estimate</span>
<span class="n">eval_only</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;gpt2-large&#39;</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-eval_gpt2_medium-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/eval_gpt2_medium.py <span class="muted">(216 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># evaluate the base gpt2</span>
<span class="c1"># n_layer=24, n_head=16, n_embd=1024</span>
<span class="c1"># 350M parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># use more iterations to get good estimate</span>
<span class="n">eval_only</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;gpt2-medium&#39;</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-eval_gpt2_xl-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/eval_gpt2_xl.py <span class="muted">(213 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># evaluate the base gpt2</span>
<span class="c1"># n_layer=48, n_head=25, n_embd=1600</span>
<span class="c1"># 1558M parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># use more iterations to get good estimate</span>
<span class="n">eval_only</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;gpt2-xl&#39;</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-finetune_shakespeare-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/finetune_shakespeare.py <span class="muted">(645 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">out_dir</span> <span class="o">=</span> <span class="s1">&#39;out-shakespeare&#39;</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># feel free to turn on</span>
<span class="n">wandb_project</span> <span class="o">=</span> <span class="s1">&#39;shakespeare&#39;</span>
<span class="n">wandb_run_name</span> <span class="o">=</span> <span class="s1">&#39;ft-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="s1">&#39;shakespeare&#39;</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;gpt2-xl&#39;</span> <span class="c1"># this is the largest GPT-2 model</span>

<span class="c1"># only save checkpoints if the validation loss improves</span>
<span class="n">always_save_checkpoint</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># the number of examples per iter:</span>
<span class="c1"># 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter</span>
<span class="c1"># shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># finetune at constant LR</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-5</span>
<span class="n">decay_lr</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-train_gpt2-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/train_gpt2.py <span class="muted">(681 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB</span>
<span class="c1"># launch as the following (e.g. in a screen session) and wait ~5 days:</span>
<span class="c1"># $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py</span>

<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wandb_project</span> <span class="o">=</span> <span class="s1">&#39;owt&#39;</span>
<span class="n">wandb_run_name</span><span class="o">=</span><span class="s1">&#39;gpt2-124M&#39;</span>

<span class="c1"># these make the total batch size be ~0.5M</span>
<span class="c1"># 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">8</span>

<span class="c1"># this makes total number of tokens be 300B</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">600000</span>
<span class="n">lr_decay_iters</span> <span class="o">=</span> <span class="mi">600000</span>

<span class="c1"># eval stuff</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># weight decay</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-1</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-config-train_shakespeare_char-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>config/train_shakespeare_char.py <span class="muted">(1.1 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># train a miniature character-level shakespeare model</span>
<span class="c1"># good for debugging and playing on macbooks and such</span>

<span class="n">out_dir</span> <span class="o">=</span> <span class="s1">&#39;out-shakespeare-char&#39;</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">250</span> <span class="c1"># keep frequent because we&#39;ll overfit</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># don&#39;t print too too often</span>

<span class="c1"># we expect to overfit on this small dataset, so only save when val improves</span>
<span class="n">always_save_checkpoint</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># override via command line if you like</span>
<span class="n">wandb_project</span> <span class="o">=</span> <span class="s1">&#39;shakespeare-char&#39;</span>
<span class="n">wandb_run_name</span> <span class="o">=</span> <span class="s1">&#39;mini-gpt&#39;</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="s1">&#39;shakespeare_char&#39;</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># context of up to 256 previous characters</span>

<span class="c1"># baby GPT model :)</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span> <span class="c1"># with baby networks can afford to go a bit higher</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">lr_decay_iters</span> <span class="o">=</span> <span class="mi">5000</span> <span class="c1"># make equal to max_iters usually</span>
<span class="n">min_lr</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># learning_rate / 10 usually</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="c1"># make a bit bigger because number of tokens per iter is small</span>

<span class="n">warmup_iters</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># not super necessary potentially</span>

<span class="c1"># on macbook also add</span>
<span class="c1"># device = &#39;cpu&#39;  # run on cpu only</span>
<span class="c1"># compile = False # do not torch compile the model</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-configurator-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>configurator.py <span class="muted">(1.7 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Poor Man&#39;s Configurator. Probably a terrible idea. Example usage:</span>
<span class="sd">$ python train.py config/override_file.py --batch_size=32</span>
<span class="sd">this will first run config/override_file.py, then override batch_size to 32</span>

<span class="sd">The code in this file will be run as follows from e.g. train.py:</span>
<span class="sd">&gt;&gt;&gt; exec(open(&#39;configurator.py&#39;).read())</span>

<span class="sd">So it&#39;s not a Python module, it&#39;s just shuttling this code away from train.py</span>
<span class="sd">The code in this script then overrides the globals()</span>

<span class="sd">I know people are not going to love this, I just really dislike configuration</span>
<span class="sd">complexity and having to prepend config. to every single variable. If someone</span>
<span class="sd">comes up with a better simple Python solution I am all ears.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ast</span><span class="w"> </span><span class="kn">import</span> <span class="n">literal_eval</span>

<span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="k">if</span> <span class="s1">&#39;=&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">arg</span><span class="p">:</span>
        <span class="c1"># assume it&#39;s the name of a config file</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
        <span class="n">config_file</span> <span class="o">=</span> <span class="n">arg</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Overriding config with </span><span class="si">{</span><span class="n">config_file</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="n">exec</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># assume it&#39;s a --key=value argument</span>
        <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="o">=</span> <span class="n">arg</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;=&#39;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># attempt to eval it it (e.g. if bool, number, or etc)</span>
                <span class="n">attempt</span> <span class="o">=</span> <span class="n">literal_eval</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">SyntaxError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
                <span class="c1"># if that goes wrong, just use the string</span>
                <span class="n">attempt</span> <span class="o">=</span> <span class="n">val</span>
            <span class="c1"># ensure the types match ok</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">attempt</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="nb">globals</span><span class="p">()[</span><span class="n">key</span><span class="p">])</span>
            <span class="c1"># cross fingers</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Overriding: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">attempt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">globals</span><span class="p">()[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attempt</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown config key: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-openwebtext-prepare-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>data/openwebtext/prepare.py <span class="muted">(3.1 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="c1"># saves the openwebtext dataset to a binary file for training. following was helpful:</span>
<span class="c1"># https://github.com/HazyResearch/flash-attention/blob/main/training/src/datamodules/language_modeling_hf.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span> <span class="c1"># huggingface datasets</span>

<span class="c1"># number of workers in .map() call</span>
<span class="c1"># good number to use is ~order number of cpu cores // 2</span>
<span class="n">num_proc</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># number of workers in load_dataset() call</span>
<span class="c1"># best number might be different from num_proc above as it also depends on NW speed.</span>
<span class="c1"># it is better than 1 usually though</span>
<span class="n">num_proc_load_dataset</span> <span class="o">=</span> <span class="n">num_proc</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;openwebtext&quot;</span><span class="p">,</span> <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc_load_dataset</span><span class="p">)</span>

    <span class="c1"># owt by default only contains the &#39;train&#39; split, so create a test split</span>
    <span class="n">split_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2357</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">split_dataset</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span> <span class="c1"># rename the test split to val</span>

    <span class="c1"># this results in:</span>
    <span class="c1"># &gt;&gt;&gt; split_dataset</span>
    <span class="c1"># DatasetDict({</span>
    <span class="c1">#     train: Dataset({</span>
    <span class="c1">#         features: [&#39;text&#39;],</span>
    <span class="c1">#         num_rows: 8009762</span>
    <span class="c1">#     })</span>
    <span class="c1">#     val: Dataset({</span>
    <span class="c1">#         features: [&#39;text&#39;],</span>
    <span class="c1">#         num_rows: 4007</span>
    <span class="c1">#     })</span>
    <span class="c1"># })</span>

    <span class="c1"># we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">process</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">encode_ordinary</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> <span class="c1"># encode_ordinary ignores any special tokens</span>
        <span class="n">ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">eot_token</span><span class="p">)</span> <span class="c1"># add the end of text token, e.g. 50256 for gpt2 bpe</span>
        <span class="c1"># note: I think eot should be prepended not appended... hmm. it&#39;s called &quot;eot&quot; though...</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;ids&#39;</span><span class="p">:</span> <span class="n">ids</span><span class="p">,</span> <span class="s1">&#39;len&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># tokenize the dataset</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">process</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;tokenizing the splits&quot;</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># concatenate all the ids in each dataset into one large file we can use for training</span>
    <span class="k">for</span> <span class="n">split</span><span class="p">,</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arr_len</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dset</span><span class="p">[</span><span class="s1">&#39;len&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint64</span><span class="p">)</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s1">.bin&#39;</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint16</span> <span class="c1"># (can do since enc.max_token_value == 50256 is &lt; 2**16)</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w+&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">arr_len</span><span class="p">,))</span>
        <span class="n">total_batches</span> <span class="o">=</span> <span class="mi">1024</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">total_batches</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;writing </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">):</span>
            <span class="c1"># Batch together samples for faster write</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">num_shards</span><span class="o">=</span><span class="n">total_batches</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">with_format</span><span class="p">(</span><span class="s1">&#39;numpy&#39;</span><span class="p">)</span>
            <span class="n">arr_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">])</span>
            <span class="c1"># Write into mmap</span>
            <span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_batch</span><span class="p">)]</span> <span class="o">=</span> <span class="n">arr_batch</span>
            <span class="n">idx</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_batch</span><span class="p">)</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="c1"># train.bin is ~17GB, val.bin ~8.5MB</span>
    <span class="c1"># train has ~9B tokens (9,035,582,198)</span>
    <span class="c1"># val has ~4M tokens (4,434,897)</span>

    <span class="c1"># to read the bin files later, e.g. with numpy:</span>
    <span class="c1"># m = np.memmap(&#39;train.bin&#39;, dtype=np.uint16, mode=&#39;r&#39;)</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-openwebtext-readme-md">
  <h2 data-icon="ğŸ“">
    <div class="file-header-left">
      <span>data/openwebtext/readme.md <span class="muted">(489 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="markdown-content"><h2 id="openwebtext-dataset">openwebtext dataset</h2>
<p>after running <code>prepare.py</code> (preprocess) we get:</p>
<ul>
<li>train.bin is ~17GB, val.bin ~8.5MB</li>
<li>train has ~9B tokens (9,035,582,198)</li>
<li>val has ~4M tokens (4,434,897)</li>
</ul>
<p>this came from 8,013,769 documents in total.</p>
<p>references:</p>
<ul>
<li>OpenAI's WebText dataset is discussed in <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 paper</a></li>
<li><a href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a> dataset</li>
</ul></div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-shakespeare-prepare-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>data/shakespeare/prepare.py <span class="muted">(1.1 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># download the tiny shakespeare dataset</span>
<span class="n">input_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;input.txt&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">):</span>
    <span class="n">data_url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#39;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_url</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mf">0.9</span><span class="p">)]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mf">0.9</span><span class="p">):]</span>

<span class="c1"># encode with tiktoken gpt2 bpe</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">train_ids</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">encode_ordinary</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">val_ids</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">encode_ordinary</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;val has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>

<span class="c1"># export to bin files</span>
<span class="n">train_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">val_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">train_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;train.bin&#39;</span><span class="p">))</span>
<span class="n">val_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;val.bin&#39;</span><span class="p">))</span>

<span class="c1"># train.bin has 301,966 tokens</span>
<span class="c1"># val.bin has 36,059 tokens</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-shakespeare-readme-md">
  <h2 data-icon="ğŸ“">
    <div class="file-header-left">
      <span>data/shakespeare/readme.md <span class="muted">(161 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="markdown-content"><h1 id="tiny-shakespeare">tiny shakespeare</h1>
<p>Tiny shakespeare, of the good old char-rnn fame :)</p>
<p>After running <code>prepare.py</code>:</p>
<ul>
<li>train.bin has 301,966 tokens</li>
<li>val.bin has 36,059 tokens</li>
</ul></div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-shakespeare_char-prepare-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>data/shakespeare_char/prepare.py <span class="muted">(2.3 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Prepare the Shakespeare dataset for character-level language modeling.</span>
<span class="sd">So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.</span>
<span class="sd">Will save train.bin, val.bin containing the ids, and meta.pkl containing the</span>
<span class="sd">encoder and decoder and some other related info.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># download the tiny shakespeare dataset</span>
<span class="n">input_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;input.txt&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">):</span>
    <span class="n">data_url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#39;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_url</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;length of dataset in characters: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># get all the unique characters that occur in this text</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;all the unique characters:&quot;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># create a mapping from characters to integers</span>
<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span> <span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span> <span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span>
<span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span> <span class="c1"># encoder: take a string, output a list of integers</span>
<span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span> <span class="c1"># decoder: take a list of integers, output a string</span>

<span class="c1"># create the train and test splits</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mf">0.9</span><span class="p">)]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mf">0.9</span><span class="p">):]</span>

<span class="c1"># encode both to integers</span>
<span class="n">train_ids</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">val_ids</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;val has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>

<span class="c1"># export to bin files</span>
<span class="n">train_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">val_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">train_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;train.bin&#39;</span><span class="p">))</span>
<span class="n">val_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;val.bin&#39;</span><span class="p">))</span>

<span class="c1"># save the meta information as well, to help us encode/decode later</span>
<span class="n">meta</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="s1">&#39;itos&#39;</span><span class="p">:</span> <span class="n">itos</span><span class="p">,</span>
    <span class="s1">&#39;stoi&#39;</span><span class="p">:</span> <span class="n">stoi</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;meta.pkl&#39;</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

<span class="c1"># length of dataset in characters:  1115394</span>
<span class="c1"># all the unique characters:</span>
<span class="c1">#  !$&amp;&#39;,-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span>
<span class="c1"># vocab size: 65</span>
<span class="c1"># train has 1003854 tokens</span>
<span class="c1"># val has 111540 tokens</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-data-shakespeare_char-readme-md">
  <h2 data-icon="ğŸ“">
    <div class="file-header-left">
      <span>data/shakespeare_char/readme.md <span class="muted">(209 B)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="markdown-content"><h1 id="tiny-shakespeare-character-level">tiny shakespeare, character-level</h1>
<p>Tiny shakespeare, of the good old char-rnn fame :) Treated on character-level.</p>
<p>After running <code>prepare.py</code>:</p>
<ul>
<li>train.bin has 1,003,854 tokens</li>
<li>val.bin has 111,540 tokens</li>
</ul></div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-model-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>model.py <span class="muted">(16.0 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Full definition of a GPT Language Model, all of it in this single file.</span>
<span class="sd">References:</span>
<span class="sd">1) the official GPT-2 TensorFlow implementation released by OpenAI:</span>
<span class="sd">https://github.com/openai/gpt-2/blob/master/src/model.py</span>
<span class="sd">2) huggingface/transformers PyTorch implementation:</span>
<span class="sd">https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; LayerNorm but with an optional bias. PyTorch doesn&#39;t support simply bias=False &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># key, query, value projections for all heads, but in a batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># output projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># regularization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="c1"># flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flash</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="p">,</span> <span class="s1">&#39;scaled_dot_product_attention&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">flash</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;</span><span class="p">)</span>
            <span class="c1"># causal mask to ensure that attention is only applied to the left in the input sequence</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">))</span>
                                        <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># batch size, sequence length, embedding dimensionality (n_embd)</span>

        <span class="c1"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)</span>

        <span class="c1"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flash</span><span class="p">:</span>
            <span class="c1"># efficient attention using Flash Attention CUDA kernels</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># manual implementation of attention</span>
            <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,:,:</span><span class="n">T</span><span class="p">,:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span> <span class="c1"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># re-assemble all head outputs side by side</span>

        <span class="c1"># output projection</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_fc</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">CausalSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GPTConfig</span><span class="p">:</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50304</span> <span class="c1"># GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency</span>
    <span class="n">n_layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GPT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">wte</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">),</span>
            <span class="n">wpe</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">),</span>
            <span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Block</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">)]),</span>
            <span class="n">ln_f</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span><span class="p">),</span>
        <span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># with weight tying when using torch.compile() some warnings get generated:</span>
        <span class="c1"># &quot;UserWarning: functional_call was passed multiple values for tied weights.</span>
        <span class="c1"># This behavior is deprecated and will be an error in future versions&quot;</span>
        <span class="c1"># not 100% sure what this is, so far seems to be harmless. TODO investigate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wte</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span> <span class="c1"># https://paperswithcode.com/method/weight-tying</span>

        <span class="c1"># init all weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span>
        <span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper</span>
        <span class="k">for</span> <span class="n">pn</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">pn</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;c_proj.weight&#39;</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">))</span>

        <span class="c1"># report number of parameters</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of parameters: </span><span class="si">%.2f</span><span class="s2">M&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_num_params</span><span class="p">()</span><span class="o">/</span><span class="mf">1e6</span><span class="p">,))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_num_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">non_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the number of parameters in the model.</span>
<span class="sd">        For non-embedding count (default), the position embeddings get subtracted.</span>
<span class="sd">        The token embeddings would too, except due to the parameter sharing these</span>
<span class="sd">        params are actually used as weights in the final layer, so we include them.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">non_embedding</span><span class="p">:</span>
            <span class="n">n_params</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wpe</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">n_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">device</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Cannot forward sequence of length </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">, block size is only </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shape (t)</span>

        <span class="c1"># forward the GPT model itself</span>
        <span class="n">tok_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wte</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># token embeddings of shape (b, t, n_embd)</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wpe</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span> <span class="c1"># position embeddings of shape (t, n_embd)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">ln_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if we are given some desired targets also calculate the loss</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># inference-time mini-optimization: only forward the lm_head on the very last position</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span> <span class="c1"># note: using list [-1] to preserve the time dim</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">crop_block_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="c1"># model surgery to decrease the block size if necessary</span>
        <span class="c1"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span>
        <span class="c1"># but want to use a smaller block size for some smaller, simpler model</span>
        <span class="k">assert</span> <span class="n">block_size</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wpe</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wpe</span><span class="o">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">block_size</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">attn</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">):</span>
                <span class="n">block</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,:,:</span><span class="n">block_size</span><span class="p">,:</span><span class="n">block_size</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">override_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;gpt2&#39;</span><span class="p">,</span> <span class="s1">&#39;gpt2-medium&#39;</span><span class="p">,</span> <span class="s1">&#39;gpt2-large&#39;</span><span class="p">,</span> <span class="s1">&#39;gpt2-xl&#39;</span><span class="p">}</span>
        <span class="n">override_args</span> <span class="o">=</span> <span class="n">override_args</span> <span class="ow">or</span> <span class="p">{}</span> <span class="c1"># default to empty dict</span>
        <span class="c1"># only dropout can be overridden see more notes below</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;dropout&#39;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">override_args</span><span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2LMHeadModel</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loading weights from pretrained gpt: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">model_type</span><span class="p">)</span>

        <span class="c1"># n_layer, n_head and n_embd are determined from model_type</span>
        <span class="n">config_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;gpt2&#39;</span><span class="p">:</span>         <span class="nb">dict</span><span class="p">(</span><span class="n">n_layer</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">768</span><span class="p">),</span>  <span class="c1"># 124M params</span>
            <span class="s1">&#39;gpt2-medium&#39;</span><span class="p">:</span>  <span class="nb">dict</span><span class="p">(</span><span class="n">n_layer</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">1024</span><span class="p">),</span> <span class="c1"># 350M params</span>
            <span class="s1">&#39;gpt2-large&#39;</span><span class="p">:</span>   <span class="nb">dict</span><span class="p">(</span><span class="n">n_layer</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">1280</span><span class="p">),</span> <span class="c1"># 774M params</span>
            <span class="s1">&#39;gpt2-xl&#39;</span><span class="p">:</span>      <span class="nb">dict</span><span class="p">(</span><span class="n">n_layer</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">1600</span><span class="p">),</span> <span class="c1"># 1558M params</span>
        <span class="p">}[</span><span class="n">model_type</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forcing vocab_size=50257, block_size=1024, bias=True&quot;</span><span class="p">)</span>
        <span class="n">config_args</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">50257</span> <span class="c1"># always 50257 for GPT model checkpoints</span>
        <span class="n">config_args</span><span class="p">[</span><span class="s1">&#39;block_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span> <span class="c1"># always 1024 for GPT model checkpoints</span>
        <span class="n">config_args</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># always True for GPT model checkpoints</span>
        <span class="c1"># we can override the dropout rate, if desired</span>
        <span class="k">if</span> <span class="s1">&#39;dropout&#39;</span> <span class="ow">in</span> <span class="n">override_args</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;overriding dropout rate to </span><span class="si">{</span><span class="n">override_args</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">config_args</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">override_args</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">]</span>
        <span class="c1"># create a from-scratch initialized minGPT model</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">config_args</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">sd_keys</span> <span class="o">=</span> <span class="n">sd</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="n">sd_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd_keys</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.attn.bias&#39;</span><span class="p">)]</span> <span class="c1"># discard this mask / buffer, not a param</span>

        <span class="c1"># init a huggingface/transformers model</span>
        <span class="n">model_hf</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_type</span><span class="p">)</span>
        <span class="n">sd_hf</span> <span class="o">=</span> <span class="n">model_hf</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># copy while ensuring all of the parameters are aligned and match in names and shapes</span>
        <span class="n">sd_keys_hf</span> <span class="o">=</span> <span class="n">sd_hf</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="n">sd_keys_hf</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd_keys_hf</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.attn.masked_bias&#39;</span><span class="p">)]</span> <span class="c1"># ignore these, just a buffer</span>
        <span class="n">sd_keys_hf</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd_keys_hf</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.attn.bias&#39;</span><span class="p">)]</span> <span class="c1"># same, just the mask (buffer)</span>
        <span class="n">transposed</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;attn.c_attn.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;attn.c_proj.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;mlp.c_fc.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;mlp.c_proj.weight&#39;</span><span class="p">]</span>
        <span class="c1"># basically the openai checkpoints use a &quot;Conv1D&quot; module, but we only want to use a vanilla Linear</span>
        <span class="c1"># this means that we have to transpose these weights when we import them</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">sd_keys_hf</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">sd_keys</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;mismatched keys: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sd_keys_hf</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sd_keys</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd_keys_hf</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">transposed</span><span class="p">):</span>
                <span class="c1"># special treatment for the Conv1D weights we need to transpose</span>
                <span class="k">assert</span> <span class="n">sd_hf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">sd_hf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># vanilla copy over the other parameters</span>
                <span class="k">assert</span> <span class="n">sd_hf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">sd_hf</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">betas</span><span class="p">,</span> <span class="n">device_type</span><span class="p">):</span>
        <span class="c1"># start with all of the candidate parameters</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">pn</span><span class="p">:</span> <span class="n">p</span> <span class="k">for</span> <span class="n">pn</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()}</span>
        <span class="c1"># filter out those that do not require grad</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">pn</span><span class="p">:</span> <span class="n">p</span> <span class="k">for</span> <span class="n">pn</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">}</span>
        <span class="c1"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span>
        <span class="c1"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#39;t.</span>
        <span class="n">decay_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">nodecay_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">optim_groups</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">decay_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="n">weight_decay</span><span class="p">},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">nodecay_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
        <span class="p">]</span>
        <span class="n">num_decay_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">decay_params</span><span class="p">)</span>
        <span class="n">num_nodecay_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">nodecay_params</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;num decayed parameter tensors: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">decay_params</span><span class="p">)</span><span class="si">}</span><span class="s2">, with </span><span class="si">{</span><span class="n">num_decay_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;num non-decayed parameter tensors: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">nodecay_params</span><span class="p">)</span><span class="si">}</span><span class="s2">, with </span><span class="si">{</span><span class="n">num_nodecay_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
        <span class="c1"># Create AdamW optimizer and use the fused version if it is available</span>
        <span class="n">fused_available</span> <span class="o">=</span> <span class="s1">&#39;fused&#39;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">use_fused</span> <span class="o">=</span> <span class="n">fused_available</span> <span class="ow">and</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span>
        <span class="n">extra_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_fused</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">optim_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using fused AdamW: </span><span class="si">{</span><span class="n">use_fused</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">estimate_mfu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fwdbwd_per_iter</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS &quot;&quot;&quot;</span>
        <span class="c1"># first estimate the number of flops we do per iteration.</span>
        <span class="c1"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_num_params</span><span class="p">()</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">n_layer</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">n_embd</span><span class="o">//</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">block_size</span>
        <span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span><span class="o">*</span><span class="n">N</span> <span class="o">+</span> <span class="mi">12</span><span class="o">*</span><span class="n">L</span><span class="o">*</span><span class="n">H</span><span class="o">*</span><span class="n">Q</span><span class="o">*</span><span class="n">T</span>
        <span class="n">flops_per_fwdbwd</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">T</span>
        <span class="n">flops_per_iter</span> <span class="o">=</span> <span class="n">flops_per_fwdbwd</span> <span class="o">*</span> <span class="n">fwdbwd_per_iter</span>
        <span class="c1"># express our flops throughput as ratio of A100 bfloat16 peak flops</span>
        <span class="n">flops_achieved</span> <span class="o">=</span> <span class="n">flops_per_iter</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">dt</span><span class="p">)</span> <span class="c1"># per second</span>
        <span class="n">flops_promised</span> <span class="o">=</span> <span class="mf">312e12</span> <span class="c1"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span>
        <span class="n">mfu</span> <span class="o">=</span> <span class="n">flops_achieved</span> <span class="o">/</span> <span class="n">flops_promised</span>
        <span class="k">return</span> <span class="n">mfu</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span>
<span class="sd">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span>
<span class="sd">        Most likely you&#39;ll want to make sure to be in model.eval() mode of operation for this.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># if the sequence context is growing too long we must crop it at block_size</span>
            <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span> <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span> <span class="k">else</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">:]</span>
            <span class="c1"># forward the model to get the logits for the index in the sequence</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
            <span class="c1"># pluck the logits at the final step and scale by desired temperature</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="c1"># optionally crop the logits to only the top k options</span>
            <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;Inf&#39;</span><span class="p">)</span>
            <span class="c1"># apply softmax to convert logits to (normalized) probabilities</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sample from the distribution</span>
            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># append sampled index to the running sequence and continue</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">idx</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-sample-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>sample.py <span class="muted">(3.8 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Sample from a trained model</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTConfig</span><span class="p">,</span> <span class="n">GPT</span>

<span class="c1"># -----------------------------------------------------------------------------</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;resume&#39;</span> <span class="c1"># either &#39;resume&#39; (from an out_dir) or a gpt2 variant (e.g. &#39;gpt2-xl&#39;)</span>
<span class="n">out_dir</span> <span class="o">=</span> <span class="s1">&#39;out&#39;</span> <span class="c1"># ignored if init_from is not &#39;resume&#39;</span>
<span class="n">start</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="c1"># or &quot;&lt;|endoftext|&gt;&quot; or etc. Can also specify a file, use as: &quot;FILE:prompt.txt&quot;</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># number of samples to draw</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># number of tokens generated in each sample</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.8</span> <span class="c1"># 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># retain only the top_k most likely tokens, clamp others to have 0 probability</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1337</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="c1"># examples: &#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, &#39;cuda:1&#39;, etc.</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;bfloat16&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;float16&#39;</span> <span class="c1"># &#39;float32&#39; or &#39;bfloat16&#39; or &#39;float16&#39;</span>
<span class="nb">compile</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># use PyTorch 2.0 to compile the model to be faster</span>
<span class="n">exec</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;configurator.py&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span> <span class="c1"># overrides from command line or config file</span>
<span class="c1"># -----------------------------------------------------------------------------</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on matmul</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on cudnn</span>
<span class="n">device_type</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">in</span> <span class="n">device</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span> <span class="c1"># for later use in torch.autocast</span>
<span class="n">ptdtype</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;float32&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;bfloat16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s1">&#39;float16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}[</span><span class="n">dtype</span><span class="p">]</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span> <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ptdtype</span><span class="p">)</span>

<span class="c1"># model</span>
<span class="k">if</span> <span class="n">init_from</span> <span class="o">==</span> <span class="s1">&#39;resume&#39;</span><span class="p">:</span>
    <span class="c1"># init from a model saved in a specific directory</span>
    <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s1">&#39;ckpt.pt&#39;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">gptconf</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">gptconf</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
    <span class="n">unwanted_prefix</span> <span class="o">=</span> <span class="s1">&#39;_orig_mod.&#39;</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">unwanted_prefix</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="n">k</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">unwanted_prefix</span><span class="p">):]]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">init_from</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">):</span>
    <span class="c1"># init from a given GPT-2 model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">init_from</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">compile</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># requires PyTorch 2.0 (optional)</span>

<span class="c1"># look for the meta pickle in case it is available in the dataset folder</span>
<span class="n">load_meta</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">init_from</span> <span class="o">==</span> <span class="s1">&#39;resume&#39;</span> <span class="ow">and</span> <span class="s1">&#39;config&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span> <span class="ow">and</span> <span class="s1">&#39;dataset&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;config&#39;</span><span class="p">]:</span> <span class="c1"># older checkpoints might not have these...</span>
    <span class="n">meta_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;config&#39;</span><span class="p">][</span><span class="s1">&#39;dataset&#39;</span><span class="p">],</span> <span class="s1">&#39;meta.pkl&#39;</span><span class="p">)</span>
    <span class="n">load_meta</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">meta_path</span><span class="p">)</span>
<span class="k">if</span> <span class="n">load_meta</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading meta from </span><span class="si">{</span><span class="n">meta_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">meta_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">meta</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="c1"># TODO want to make this more general to arbitrary encoder/decoder schemes</span>
    <span class="n">stoi</span><span class="p">,</span> <span class="n">itos</span> <span class="o">=</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;stoi&#39;</span><span class="p">],</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;itos&#39;</span><span class="p">]</span>
    <span class="n">encode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
    <span class="n">decode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># ok let&#39;s assume gpt-2 encodings by default</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No meta.pkl found, assuming GPT-2 encodings...&quot;</span><span class="p">)</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
    <span class="n">encode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">enc</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;&lt;|endoftext|&gt;&quot;</span><span class="p">})</span>
    <span class="n">decode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">enc</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="c1"># encode the beginning of the prompt</span>
<span class="k">if</span> <span class="n">start</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;FILE:&#39;</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="mi">5</span><span class="p">:],</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">start_ids</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">start_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>

<span class="c1"># run generation</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">decode</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------&#39;</span><span class="p">)</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-train-py">
  <h2 data-icon="ğŸ">
    <div class="file-header-left">
      <span>train.py <span class="muted">(14.5 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This training script can be run both on a single gpu in debug mode,</span>
<span class="sd">and also in a larger training run with distributed data parallel (ddp).</span>

<span class="sd">To run on a single GPU, example:</span>
<span class="sd">$ python train.py --batch_size=32 --compile=False</span>

<span class="sd">To run with DDP on 4 gpus on 1 node, example:</span>
<span class="sd">$ torchrun --standalone --nproc_per_node=4 train.py</span>

<span class="sd">To run with DDP on 4 gpus across 2 nodes, example:</span>
<span class="sd">- Run on the first (master) node with example IP 123.456.123.456:</span>
<span class="sd">$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py</span>
<span class="sd">- Run on the worker node:</span>
<span class="sd">$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py</span>
<span class="sd">(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">nullcontext</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_process_group</span><span class="p">,</span> <span class="n">destroy_process_group</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTConfig</span><span class="p">,</span> <span class="n">GPT</span>

<span class="c1"># -----------------------------------------------------------------------------</span>
<span class="c1"># default config values designed to train a gpt2 (124M) on OpenWebText</span>
<span class="c1"># I/O</span>
<span class="n">out_dir</span> <span class="o">=</span> <span class="s1">&#39;out&#39;</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">eval_only</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># if True, script exits right after the first eval</span>
<span class="n">always_save_checkpoint</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># if True, always save a checkpoint after each eval</span>
<span class="n">init_from</span> <span class="o">=</span> <span class="s1">&#39;scratch&#39;</span> <span class="c1"># &#39;scratch&#39; or &#39;resume&#39; or &#39;gpt2*&#39;</span>
<span class="c1"># wandb logging</span>
<span class="n">wandb_log</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># disabled by default</span>
<span class="n">wandb_project</span> <span class="o">=</span> <span class="s1">&#39;owt&#39;</span>
<span class="n">wandb_run_name</span> <span class="o">=</span> <span class="s1">&#39;gpt2&#39;</span> <span class="c1"># &#39;run&#39; + str(time.time())</span>
<span class="c1"># data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="s1">&#39;openwebtext&#39;</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">8</span> <span class="c1"># used to simulate larger batch sizes</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">12</span> <span class="c1"># if gradient_accumulation_steps &gt; 1, this is the micro-batch size</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="c1"># model</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">768</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># for pretraining 0 is good, for finetuning try 0.1+</span>
<span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># do we use bias inside LayerNorm and Linear layers?</span>
<span class="c1"># adamw optimizer</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">6e-4</span> <span class="c1"># max learning rate</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">600000</span> <span class="c1"># total number of training iterations</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">grad_clip</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># clip gradients at this value, or disable if == 0.0</span>
<span class="c1"># learning rate decay settings</span>
<span class="n">decay_lr</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># whether to decay the learning rate</span>
<span class="n">warmup_iters</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c1"># how many steps to warm up for</span>
<span class="n">lr_decay_iters</span> <span class="o">=</span> <span class="mi">600000</span> <span class="c1"># should be ~= max_iters per Chinchilla</span>
<span class="n">min_lr</span> <span class="o">=</span> <span class="mf">6e-5</span> <span class="c1"># minimum learning rate, should be ~= learning_rate/10 per Chinchilla</span>
<span class="c1"># DDP settings</span>
<span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span> <span class="c1"># &#39;nccl&#39;, &#39;gloo&#39;, etc.</span>
<span class="c1"># system</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="c1"># examples: &#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, &#39;cuda:1&#39; etc., or try &#39;mps&#39; on macbooks</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;bfloat16&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;float16&#39;</span> <span class="c1"># &#39;float32&#39;, &#39;bfloat16&#39;, or &#39;float16&#39;, the latter will auto implement a GradScaler</span>
<span class="nb">compile</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># use PyTorch 2.0 to compile the model to be faster</span>
<span class="c1"># -----------------------------------------------------------------------------</span>
<span class="n">config_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">))]</span>
<span class="n">exec</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;configurator.py&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span> <span class="c1"># overrides from command line or config file</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">globals</span><span class="p">()[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">config_keys</span><span class="p">}</span> <span class="c1"># will be useful for logging</span>
<span class="c1"># -----------------------------------------------------------------------------</span>

<span class="c1"># various inits, derived attributes, I/O setup</span>
<span class="n">ddp</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;RANK&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># is this a ddp run?</span>
<span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
    <span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">ddp_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;RANK&#39;</span><span class="p">])</span>
    <span class="n">ddp_local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LOCAL_RANK&#39;</span><span class="p">])</span>
    <span class="n">ddp_world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;WORLD_SIZE&#39;</span><span class="p">])</span>
    <span class="n">device</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">ddp_local_rank</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">master_process</span> <span class="o">=</span> <span class="n">ddp_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># this process will do logging, checkpointing etc.</span>
    <span class="n">seed_offset</span> <span class="o">=</span> <span class="n">ddp_rank</span> <span class="c1"># each process gets a different seed</span>
    <span class="c1"># world_size number of processes will be training simultaneously, so we can scale</span>
    <span class="c1"># down the desired gradient accumulation iterations per process proportionally</span>
    <span class="k">assert</span> <span class="n">gradient_accumulation_steps</span> <span class="o">%</span> <span class="n">ddp_world_size</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">gradient_accumulation_steps</span> <span class="o">//=</span> <span class="n">ddp_world_size</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># if not ddp, we are running on a single gpu, and one process</span>
    <span class="n">master_process</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">seed_offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ddp_world_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">tokens_per_iter</span> <span class="o">=</span> <span class="n">gradient_accumulation_steps</span> <span class="o">*</span> <span class="n">ddp_world_size</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">block_size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;tokens per iteration will be: </span><span class="si">{</span><span class="n">tokens_per_iter</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">master_process</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1337</span> <span class="o">+</span> <span class="n">seed_offset</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on matmul</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># allow tf32 on cudnn</span>
<span class="n">device_type</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">in</span> <span class="n">device</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span> <span class="c1"># for later use in torch.autocast</span>
<span class="c1"># note: float16 data type will automatically use a GradScaler</span>
<span class="n">ptdtype</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;float32&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;bfloat16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s1">&#39;float16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}[</span><span class="n">dtype</span><span class="p">]</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span> <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ptdtype</span><span class="p">)</span>

<span class="c1"># poor man&#39;s data loader</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="c1"># We recreate np.memmap every batch to avoid a memory leak, as per</span>
    <span class="c1"># https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122</span>
    <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;train.bin&#39;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;val.bin&#39;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">+</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
        <span class="c1"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># init these up here, can override if init_from=&#39;resume&#39; (i.e. from a checkpoint)</span>
<span class="n">iter_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_val_loss</span> <span class="o">=</span> <span class="mf">1e9</span>

<span class="c1"># attempt to derive vocab_size from the dataset</span>
<span class="n">meta_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;meta.pkl&#39;</span><span class="p">)</span>
<span class="n">meta_vocab_size</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">meta_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">meta_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">meta</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">meta_vocab_size</span> <span class="o">=</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;found vocab_size = </span><span class="si">{</span><span class="n">meta_vocab_size</span><span class="si">}</span><span class="s2"> (inside </span><span class="si">{</span><span class="n">meta_path</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># model init</span>
<span class="n">model_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layer</span><span class="o">=</span><span class="n">n_layer</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
                  <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span> <span class="c1"># start with model_args from command line</span>
<span class="k">if</span> <span class="n">init_from</span> <span class="o">==</span> <span class="s1">&#39;scratch&#39;</span><span class="p">:</span>
    <span class="c1"># init a new model from scratch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing a new model from scratch&quot;</span><span class="p">)</span>
    <span class="c1"># determine the vocab size we&#39;ll use for from-scratch training</span>
    <span class="k">if</span> <span class="n">meta_vocab_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)&quot;</span><span class="p">)</span>
    <span class="n">model_args</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">meta_vocab_size</span> <span class="k">if</span> <span class="n">meta_vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">50304</span>
    <span class="n">gptconf</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">model_args</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">gptconf</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">init_from</span> <span class="o">==</span> <span class="s1">&#39;resume&#39;</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming training from </span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># resume training from a checkpoint.</span>
    <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s1">&#39;ckpt.pt&#39;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">checkpoint_model_args</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">]</span>
    <span class="c1"># force these config attributes to be equal otherwise we can&#39;t even resume training</span>
    <span class="c1"># the rest of the attributes (e.g. dropout) can stay as desired from command line</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;n_layer&#39;</span><span class="p">,</span> <span class="s1">&#39;n_head&#39;</span><span class="p">,</span> <span class="s1">&#39;n_embd&#39;</span><span class="p">,</span> <span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;vocab_size&#39;</span><span class="p">]:</span>
        <span class="n">model_args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoint_model_args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="c1"># create the model</span>
    <span class="n">gptconf</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">model_args</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">gptconf</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
    <span class="c1"># fix the keys of the state dictionary :(</span>
    <span class="c1"># honestly no idea how checkpoints sometimes get this prefix, have to debug more</span>
    <span class="n">unwanted_prefix</span> <span class="o">=</span> <span class="s1">&#39;_orig_mod.&#39;</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">unwanted_prefix</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="n">k</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">unwanted_prefix</span><span class="p">):]]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="n">iter_num</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;iter_num&#39;</span><span class="p">]</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;best_val_loss&#39;</span><span class="p">]</span>
<span class="k">elif</span> <span class="n">init_from</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initializing from OpenAI GPT-2 weights: </span><span class="si">{</span><span class="n">init_from</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># initialize from OpenAI GPT-2 weights</span>
    <span class="n">override_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">init_from</span><span class="p">,</span> <span class="n">override_args</span><span class="p">)</span>
    <span class="c1"># read off the created config params, so we can store them into checkpoint correctly</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;n_layer&#39;</span><span class="p">,</span> <span class="s1">&#39;n_head&#39;</span><span class="p">,</span> <span class="s1">&#39;n_embd&#39;</span><span class="p">,</span> <span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;vocab_size&#39;</span><span class="p">]:</span>
        <span class="n">model_args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="c1"># crop down the model block size if desired, using model surgery</span>
<span class="k">if</span> <span class="n">block_size</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">crop_block_size</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
    <span class="n">model_args</span><span class="p">[</span><span class="s1">&#39;block_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">block_size</span> <span class="c1"># so that the checkpoint will have the right value</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># initialize a GradScaler. If enabled=False scaler is a no-op</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="p">(</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;float16&#39;</span><span class="p">))</span>

<span class="c1"># optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">configure_optimizers</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">),</span> <span class="n">device_type</span><span class="p">)</span>
<span class="k">if</span> <span class="n">init_from</span> <span class="o">==</span> <span class="s1">&#39;resume&#39;</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># free up memory</span>

<span class="c1"># compile the model</span>
<span class="k">if</span> <span class="nb">compile</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;compiling the model... (takes a ~minute)&quot;</span><span class="p">)</span>
    <span class="n">unoptimized_model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># requires PyTorch 2.0</span>

<span class="c1"># wrap model into DDP container</span>
<span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">ddp_local_rank</span><span class="p">])</span>

<span class="c1"># helps estimate an arbitrarily accurate loss over either split using many batches</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">estimate_loss</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">]:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">):</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">losses</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="c1"># learning rate decay scheduler (cosine with warmup)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="n">it</span><span class="p">):</span>
    <span class="c1"># 1) linear warmup for warmup_iters steps</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">warmup_iters</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">it</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">warmup_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># 2) if it &gt; lr_decay_iters, return min learning rate</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">&gt;</span> <span class="n">lr_decay_iters</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">min_lr</span>
    <span class="c1"># 3) in between, use cosine decay down to min learning rate</span>
    <span class="n">decay_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">it</span> <span class="o">-</span> <span class="n">warmup_iters</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">lr_decay_iters</span> <span class="o">-</span> <span class="n">warmup_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">decay_ratio</span> <span class="o">&lt;=</span> <span class="mi">1</span>
    <span class="n">coeff</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">decay_ratio</span><span class="p">))</span> <span class="c1"># coeff ranges 0..1</span>
    <span class="k">return</span> <span class="n">min_lr</span> <span class="o">+</span> <span class="n">coeff</span> <span class="o">*</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">-</span> <span class="n">min_lr</span><span class="p">)</span>

<span class="c1"># logging</span>
<span class="k">if</span> <span class="n">wandb_log</span> <span class="ow">and</span> <span class="n">master_process</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">wandb</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">wandb_project</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">wandb_run_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># training loop</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span> <span class="c1"># fetch the very first batch</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">local_iter_num</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># number of iterations in the lifetime of this process</span>
<span class="n">raw_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="n">ddp</span> <span class="k">else</span> <span class="n">model</span> <span class="c1"># unwrap DDP container if needed</span>
<span class="n">running_mfu</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

    <span class="c1"># determine and set the learning rate for this iteration</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr</span><span class="p">(</span><span class="n">iter_num</span><span class="p">)</span> <span class="k">if</span> <span class="n">decay_lr</span> <span class="k">else</span> <span class="n">learning_rate</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="c1"># evaluate the loss on train/val sets and write checkpoints</span>
    <span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">eval_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">master_process</span><span class="p">:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">estimate_loss</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;step </span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s2">: train loss </span><span class="si">{</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val loss </span><span class="si">{</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">wandb_log</span><span class="p">:</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span>
                <span class="s2">&quot;iter&quot;</span><span class="p">:</span> <span class="n">iter_num</span><span class="p">,</span>
                <span class="s2">&quot;train/loss&quot;</span><span class="p">:</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span>
                <span class="s2">&quot;val/loss&quot;</span><span class="p">:</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">],</span>
                <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
                <span class="s2">&quot;mfu&quot;</span><span class="p">:</span> <span class="n">running_mfu</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># convert to percentage</span>
            <span class="p">})</span>
        <span class="k">if</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span> <span class="ow">or</span> <span class="n">always_save_checkpoint</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">iter_num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">raw_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s1">&#39;model_args&#39;</span><span class="p">:</span> <span class="n">model_args</span><span class="p">,</span>
                    <span class="s1">&#39;iter_num&#39;</span><span class="p">:</span> <span class="n">iter_num</span><span class="p">,</span>
                    <span class="s1">&#39;best_val_loss&#39;</span><span class="p">:</span> <span class="n">best_val_loss</span><span class="p">,</span>
                    <span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;saving checkpoint to </span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="s1">&#39;ckpt.pt&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">eval_only</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># forward backward update, with optional gradient accumulation to simulate larger batch size</span>
    <span class="c1"># and using the GradScaler if data type is float16</span>
    <span class="k">for</span> <span class="n">micro_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_accumulation_steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
            <span class="c1"># in DDP training we only need to sync gradients at the last micro step.</span>
            <span class="c1"># the official way to do this is with model.no_sync() context manager, but</span>
            <span class="c1"># I really dislike that this bloats the code and forces us to repeat code</span>
            <span class="c1"># looking at the source of that context manager, it just toggles this variable</span>
            <span class="n">model</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="p">(</span><span class="n">micro_step</span> <span class="o">==</span> <span class="n">gradient_accumulation_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">gradient_accumulation_steps</span> <span class="c1"># scale the loss to account for gradient accumulation</span>
        <span class="c1"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
        <span class="c1"># backward pass, with gradient scaling if training in fp16</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># clip the gradient</span>
    <span class="k">if</span> <span class="n">grad_clip</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">grad_clip</span><span class="p">)</span>
    <span class="c1"># step the optimizer and scaler if training in fp16</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
    <span class="c1"># flush the gradients as soon as we can, no need for this memory anymore</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># timing and logging</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">t1</span>
    <span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">master_process</span><span class="p">:</span>
        <span class="c1"># get loss as float. note: this is a CPU-GPU sync point</span>
        <span class="c1"># scale up to undo the division above, approximating the true total loss (exact would have been a sum)</span>
        <span class="n">lossf</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">gradient_accumulation_steps</span>
        <span class="k">if</span> <span class="n">local_iter_num</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span> <span class="c1"># let the training loop settle a bit</span>
            <span class="n">mfu</span> <span class="o">=</span> <span class="n">raw_model</span><span class="o">.</span><span class="n">estimate_mfu</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">gradient_accumulation_steps</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
            <span class="n">running_mfu</span> <span class="o">=</span> <span class="n">mfu</span> <span class="k">if</span> <span class="n">running_mfu</span> <span class="o">==</span> <span class="o">-</span><span class="mf">1.0</span> <span class="k">else</span> <span class="mf">0.9</span><span class="o">*</span><span class="n">running_mfu</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">mfu</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;iter </span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s2">: loss </span><span class="si">{</span><span class="n">lossf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, time </span><span class="si">{</span><span class="n">dt</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms, mfu </span><span class="si">{</span><span class="n">running_mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="n">iter_num</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">local_iter_num</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># termination conditions</span>
    <span class="k">if</span> <span class="n">iter_num</span> <span class="o">&gt;</span> <span class="n">max_iters</span><span class="p">:</span>
        <span class="k">break</span>

<span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
    <span class="n">destroy_process_group</span><span class="p">()</span>
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

<section class="file-section" id="file-transformer_sizing-ipynb">
  <h2 data-icon="ğŸ“„">
    <div class="file-header-left">
      <span>transformer_sizing.ipynb <span class="muted">(14.2 KiB)</span></span>
    </div>
  </h2>
  <div class="file-body"><div class="highlight"><div class="highlight"><pre><span></span>{
 &quot;cells&quot;: [
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### Transformer Theoretical Model\n&quot;,
    &quot;\n&quot;,
    &quot;This notebook stores a bunch of analysis about a Transformer, e.g. estimates the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 1,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from collections import OrderedDict&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 2,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# config_args = {\n&quot;,
    &quot;#     &#39;gpt2&#39;:         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n&quot;,
    &quot;#     &#39;gpt2-medium&#39;:  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n&quot;,
    &quot;#     &#39;gpt2-large&#39;:   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n&quot;,
    &quot;#     &#39;gpt2-xl&#39;:      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n&quot;,
    &quot;# }[model_type]\n&quot;,
    &quot;\n&quot;,
    &quot;block_size = 1024\n&quot;,
    &quot;vocab_size = 50257\n&quot;,
    &quot;n_layer = 12\n&quot;,
    &quot;n_head = 12\n&quot;,
    &quot;n_embd = 768\n&quot;,
    &quot;bias = False\n&quot;,
    &quot;assert not bias, \&quot;this notebook assumes bias=False just for simplicity\&quot;&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 3,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;we see: 124337664, expected: 124337664, match: True\n&quot;,
      &quot;name                 params     ratio (%) \n&quot;,
      &quot;emebedding/position      786432     0.6325\n&quot;,
      &quot;embedding/token        38597376    31.0424\n&quot;,
      &quot;embedding              39383808    31.6749\n&quot;,
      &quot;attention/ln                768     0.0006\n&quot;,
      &quot;attention/kqv           1769472     1.4231\n&quot;,
      &quot;attention/proj           589824     0.4744\n&quot;,
      &quot;attention               2360064     1.8981\n&quot;,
      &quot;mlp/ln                      768     0.0006\n&quot;,
      &quot;mlp/ffw                 2359296     1.8975\n&quot;,
      &quot;mlp/proj                2359296     1.8975\n&quot;,
      &quot;mlp                     4719360     3.7956\n&quot;,
      &quot;block                   7079424     5.6937\n&quot;,
      &quot;transformer            84953088    68.3245\n&quot;,
      &quot;ln_f                        768     0.0006\n&quot;,
      &quot;dense                         0     0.0000\n&quot;,
      &quot;total                 124337664   100.0000\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;def params():\n&quot;,
    &quot;    \&quot;\&quot;\&quot; estimates the number of parameters in the model\&quot;\&quot;\&quot;\n&quot;,
    &quot;    out = OrderedDict()\n&quot;,
    &quot;\n&quot;,
    &quot;    # token and position embeddings\n&quot;,
    &quot;    out[&#39;emebedding/position&#39;] = n_embd * block_size\n&quot;,
    &quot;    out[&#39;embedding/token&#39;] = n_embd * vocab_size\n&quot;,
    &quot;    out[&#39;embedding&#39;] = out[&#39;emebedding/position&#39;] + out[&#39;embedding/token&#39;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # attention blocks\n&quot;,
    &quot;    out[&#39;attention/ln&#39;] = n_embd # note, bias=False in our LN\n&quot;,
    &quot;    out[&#39;attention/kqv&#39;] = n_embd * 3*n_embd\n&quot;,
    &quot;    out[&#39;attention/proj&#39;] = n_embd**2\n&quot;,
    &quot;    out[&#39;attention&#39;] = out[&#39;attention/ln&#39;] + out[&#39;attention/kqv&#39;] + out[&#39;attention/proj&#39;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # MLP blocks\n&quot;,
    &quot;    ffw_size = 4*n_embd # feed forward size\n&quot;,
    &quot;    out[&#39;mlp/ln&#39;] = n_embd\n&quot;,
    &quot;    out[&#39;mlp/ffw&#39;] = n_embd * ffw_size\n&quot;,
    &quot;    out[&#39;mlp/proj&#39;] = ffw_size * n_embd\n&quot;,
    &quot;    out[&#39;mlp&#39;] = out[&#39;mlp/ln&#39;] + out[&#39;mlp/ffw&#39;] + out[&#39;mlp/proj&#39;]\n&quot;,
    &quot;    \n&quot;,
    &quot;    # the transformer and the rest of it\n&quot;,
    &quot;    out[&#39;block&#39;] = out[&#39;attention&#39;] + out[&#39;mlp&#39;]\n&quot;,
    &quot;    out[&#39;transformer&#39;] = n_layer * out[&#39;block&#39;]\n&quot;,
    &quot;    out[&#39;ln_f&#39;] = n_embd # final layernorm\n&quot;,
    &quot;    out[&#39;dense&#39;] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n&quot;,
    &quot;\n&quot;,
    &quot;    # total\n&quot;,
    &quot;    out[&#39;total&#39;] = out[&#39;embedding&#39;] + out[&#39;transformer&#39;] + out[&#39;ln_f&#39;] + out[&#39;dense&#39;]\n&quot;,
    &quot;\n&quot;,
    &quot;    return out\n&quot;,
    &quot;\n&quot;,
    &quot;# compare our param count to that reported by PyTorch\n&quot;,
    &quot;p = params()\n&quot;,
    &quot;params_total = p[&#39;total&#39;]\n&quot;,
    &quot;print(f\&quot;we see: {params_total}, expected: {124337664}, match: {params_total == 124337664}\&quot;)\n&quot;,
    &quot;# create a header\n&quot;,
    &quot;print(f\&quot;{&#39;name&#39;:20s} {&#39;params&#39;:10s} {&#39;ratio (%)&#39;:10s}\&quot;)\n&quot;,
    &quot;for k,v in p.items():\n&quot;,
    &quot;    print(f\&quot;{k:20s} {v:10d} {v/params_total*100:10.4f}\&quot;)\n&quot;,
    &quot;    &quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 4,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;est checkpoint size: 1.49 GB\n&quot;,
      &quot;measured with wc -c ckpt.pt: 1542470366\n&quot;,
      &quot;fluff ratio: 103.38%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# we can now calculate the size of each checkpoint\n&quot;,
    &quot;# params are stored in fp32, and the AdamW optimizer has 2 additional buffers per param for statistics\n&quot;,
    &quot;params_bytes = params_total*4\n&quot;,
    &quot;params_and_buffers_bytes = params_bytes + 2*params_bytes\n&quot;,
    &quot;print(f\&quot;est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\&quot;)\n&quot;,
    &quot;measured_bytes = 1542470366 # from wc -c ckpt.pt\n&quot;,
    &quot;print(f\&quot;measured with wc -c ckpt.pt: {measured_bytes}\&quot;)\n&quot;,
    &quot;print(f\&quot;fluff ratio: {measured_bytes/params_and_buffers_bytes*100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We can also estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 5,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;memory ratio taken up just for parameters: 3.73%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;gpu_memory = 40e9 # 40 GB A100 GPU, roughly\n&quot;,
    &quot;print(f\&quot;memory ratio taken up just for parameters: {params_and_buffers_bytes / gpu_memory * 100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;i.e. not that much of the memory for this tiny model, most of the memory is activations (forward and backward). This of course changes dramatically for larger and larger models.&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Let&#39;s estimate FLOPs for a single forward pass.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 6,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;name                 flops          ratio (%) \n&quot;,
      &quot;attention/kqv            3623878656     1.2426\n&quot;,
      &quot;attention/scores         1610612736     0.5522\n&quot;,
      &quot;attention/reduce         1610612736     0.5522\n&quot;,
      &quot;attention/proj           1207959552     0.4142\n&quot;,
      &quot;attention                8053063680     2.7612\n&quot;,
      &quot;mlp/ffw1                 4831838208     1.6567\n&quot;,
      &quot;mlp/ffw2                 4831838208     1.6567\n&quot;,
      &quot;mlp                      9663676416     3.3135\n&quot;,
      &quot;block                   17716740096     6.0747\n&quot;,
      &quot;transformer            212600881152    72.8963\n&quot;,
      &quot;dense                   79047426048    27.1037\n&quot;,
      &quot;forward_total          291648307200   100.0000\n&quot;,
      &quot;backward_total         583296614400   200.0000\n&quot;,
      &quot;total                  874944921600   300.0000\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;def flops():\n&quot;,
    &quot;    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n&quot;,
    &quot;    # we count actual FLOPs, not MACs. Hence 2* all over the place\n&quot;,
    &quot;    # basically for any matrix multiply A (BxC) @ B (CxD) -&gt; (BxD) flops are 2*B*C*D\n&quot;,
    &quot;\n&quot;,
    &quot;    out = OrderedDict()\n&quot;,
    &quot;    head_size = n_embd // n_head\n&quot;,
    &quot;\n&quot;,
    &quot;    # attention blocks\n&quot;,
    &quot;    # 1) the projection to key, query, values\n&quot;,
    &quot;    out[&#39;attention/kqv&#39;] = 2 * block_size * (n_embd * 3*n_embd)\n&quot;,
    &quot;    # 2) calculating the attention scores\n&quot;,
    &quot;    out[&#39;attention/scores&#39;] = 2 * block_size * block_size * n_embd\n&quot;,
    &quot;    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n&quot;,
    &quot;    out[&#39;attention/reduce&#39;] = 2 * n_head * (block_size * block_size * head_size)\n&quot;,
    &quot;    # 4) the final linear projection\n&quot;,
    &quot;    out[&#39;attention/proj&#39;] = 2 * block_size * (n_embd * n_embd)\n&quot;,
    &quot;    out[&#39;attention&#39;] = sum(out[&#39;attention/&#39;+k] for k in [&#39;kqv&#39;, &#39;scores&#39;, &#39;reduce&#39;, &#39;proj&#39;])\n&quot;,
    &quot;\n&quot;,
    &quot;    # MLP blocks\n&quot;,
    &quot;    ffw_size = 4*n_embd # feed forward size\n&quot;,
    &quot;    out[&#39;mlp/ffw1&#39;] = 2 * block_size * (n_embd * ffw_size)\n&quot;,
    &quot;    out[&#39;mlp/ffw2&#39;] = 2 * block_size * (ffw_size * n_embd)\n&quot;,
    &quot;    out[&#39;mlp&#39;] = out[&#39;mlp/ffw1&#39;] + out[&#39;mlp/ffw2&#39;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # the transformer and the rest of it\n&quot;,
    &quot;    out[&#39;block&#39;] = out[&#39;attention&#39;] + out[&#39;mlp&#39;]\n&quot;,
    &quot;    out[&#39;transformer&#39;] = n_layer * out[&#39;block&#39;]\n&quot;,
    &quot;    out[&#39;dense&#39;] = 2 * block_size * (n_embd * vocab_size)\n&quot;,
    &quot;\n&quot;,
    &quot;    # forward,backward,total\n&quot;,
    &quot;    out[&#39;forward_total&#39;] = out[&#39;transformer&#39;] + out[&#39;dense&#39;]\n&quot;,
    &quot;    out[&#39;backward_total&#39;] = 2 * out[&#39;forward_total&#39;] # use common estimate of bwd = 2*fwd\n&quot;,
    &quot;    out[&#39;total&#39;] = out[&#39;forward_total&#39;] + out[&#39;backward_total&#39;]\n&quot;,
    &quot;\n&quot;,
    &quot;    return out\n&quot;,
    &quot;    \n&quot;,
    &quot;# compare our param count to that reported by PyTorch\n&quot;,
    &quot;f = flops()\n&quot;,
    &quot;flops_total = f[&#39;forward_total&#39;]\n&quot;,
    &quot;print(f\&quot;{&#39;name&#39;:20s} {&#39;flops&#39;:14s} {&#39;ratio (%)&#39;:10s}\&quot;)\n&quot;,
    &quot;for k,v in f.items():\n&quot;,
    &quot;    print(f\&quot;{k:20s} {v:14d} {v/flops_total*100:10.4f}\&quot;)\n&quot;,
    &quot;    &quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 7,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;palm_flops: 875062886400, flops: 874944921600, ratio: 1.0001\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# now here is an estimate copy pasted from the PaLM paper\n&quot;,
    &quot;# this formula is often used to calculate MFU (model flops utilization)\n&quot;,
    &quot;def palm_flops():\n&quot;,
    &quot;    \&quot;\&quot;\&quot;estimate of the model flops following PaLM paper formula\&quot;\&quot;\&quot;\n&quot;,
    &quot;    # non-embedding model parameters. note that we do not subtract the\n&quot;,
    &quot;    # embedding/token params because those are tied and get used in the last layer.\n&quot;,
    &quot;    N = params()[&#39;total&#39;] - params()[&#39;emebedding/position&#39;]\n&quot;,
    &quot;    L, H, Q, T = n_layer, n_head, n_embd//n_head, block_size\n&quot;,
    &quot;    mf_per_token = 6*N + 12*L*H*Q*T\n&quot;,
    &quot;    mf = mf_per_token * block_size\n&quot;,
    &quot;    return mf\n&quot;,
    &quot;\n&quot;,
    &quot;print(f\&quot;palm_flops: {palm_flops():d}, flops: {flops()[&#39;total&#39;]:d}, ratio: {palm_flops()/flops()[&#39;total&#39;]:.4f}\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Ok they are quite similar, giving some confidence that my math in flops() function was ~ok. Now, A100 is cited at 312TFLOPS bfloat16 on tensor cores. So what is our model flops utilization (MFU)? I trained the model above with a batch_size of 20 and grad_accum of 5, which runs in about 755ms on a single A100 GPU. We get:&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 8,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;fraction of A100 used: 37.14%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# here is what we currently roughly measure\n&quot;,
    &quot;batch_size = 20 * 5 # 5 is grad_accum, so total batch size is 100\n&quot;,
    &quot;measured_time = 0.755 # in seconds per iteration\n&quot;,
    &quot;measured_throughput = batch_size / measured_time\n&quot;,
    &quot;flops_achieved = f[&#39;total&#39;] * measured_throughput\n&quot;,
    &quot;\n&quot;,
    &quot;# A100 is cited to be 312 TFLOPS of bloat16 running on tensor cores\n&quot;,
    &quot;a100_flops_promised = 312e12\n&quot;,
    &quot;\n&quot;,
    &quot;# the fraction of the A100 that we are using:\n&quot;,
    &quot;print(f\&quot;fraction of A100 used: {flops_achieved / a100_flops_promised * 100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;For reference, we&#39;d prefer to be somewhere around 50%+, and not just for a single GPU but for an entire DDP run. So we still have some work to do, but at least we&#39;re within a factor of ~2X of what is achievable with this GPU.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 9,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;time needed to train the model: 3.46 days\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# Finally let&#39;s check out the 6ND approximation as total cost of training in FLOPs\n&quot;,
    &quot;model_size = params()[&#39;total&#39;] # this is number of parameters, N\n&quot;,
    &quot;tokens_num = 300e9 # 300B tokens, this is dataset size in tokens, D\n&quot;,
    &quot;a100_flops = 312e12 # 312 TFLOPS\n&quot;,
    &quot;assumed_mfu = 0.3 # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n&quot;,
    &quot;flops_throughput = a100_flops * 8 * assumed_mfu # assume an 8XA100 node at 30% utilization\n&quot;,
    &quot;flops_needed = 6 * model_size * tokens_num # 6ND\n&quot;,
    &quot;time_needed_s = flops_needed / flops_throughput # in seconds\n&quot;,
    &quot;print(f\&quot;time needed to train the model: {time_needed_s/3600/24:.2f} days\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;This is not a bad estimate at all. I trained this model and it converged in roughly 4 days. Btw as a good reference for where 6ND comes from and some intuition around it I recommend [Dzmitry&#39;s post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4).&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Now, FLOPs are just one constraint, the other that we have to keep a close track of is the memory bandwidth. TODO estimate LOAD/STORE costs of our model later.&quot;
   ]
  }
 ],
 &quot;metadata&quot;: {
  &quot;kernelspec&quot;: {
   &quot;display_name&quot;: &quot;pytorch2&quot;,
   &quot;language&quot;: &quot;python&quot;,
   &quot;name&quot;: &quot;python3&quot;
  },
  &quot;language_info&quot;: {
   &quot;codemirror_mode&quot;: {
    &quot;name&quot;: &quot;ipython&quot;,
    &quot;version&quot;: 3
   },
   &quot;file_extension&quot;: &quot;.py&quot;,
   &quot;mimetype&quot;: &quot;text/x-python&quot;,
   &quot;name&quot;: &quot;python&quot;,
   &quot;nbconvert_exporter&quot;: &quot;python&quot;,
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
   &quot;version&quot;: &quot;3.10.8&quot;
  },
  &quot;orig_nbformat&quot;: 4,
  &quot;vscode&quot;: {
   &quot;interpreter&quot;: {
    &quot;hash&quot;: &quot;7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222&quot;
   }
  }
 },
 &quot;nbformat&quot;: 4,
 &quot;nbformat_minor&quot;: 2
}
</pre></div>
</div></div>
  <div class="back-top"><a href="#top">â†‘ Back to top</a></div>
</section>

      </div>
    </div>

    <div id="llm-view">
      <div class="content-section">
        <h2>ğŸ¤– LLM-Optimized View</h2>
        <p style="margin-bottom: 1.5rem; color: var(--text-secondary); line-height: 1.6;">
          This view presents the repository content in CXML format, optimized for Large Language Model analysis. 
          Simply copy the content below and paste it into your preferred LLM interface.
        </p>
        <textarea id="llm-text" readonly>&lt;documents&gt;
&lt;document index=&quot;1&quot;&gt;
&lt;source&gt;.gitattributes&lt;/source&gt;
&lt;document_content&gt;
# Override jupyter in Github language stats for more accurate estimate of repo code languages
# reference: https://github.com/github/linguist/blob/master/docs/overrides.md#generated-code
*.ipynb linguist-generated

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;2&quot;&gt;
&lt;source&gt;.gitignore&lt;/source&gt;
&lt;document_content&gt;
.DS_Store
.idea
.ipynb_checkpoints/
.vscode
__pycache__/
*.bin
*.pkl
*.pt
*.pyc
input.txt
env/
venv/
&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;3&quot;&gt;
&lt;source&gt;LICENSE&lt;/source&gt;
&lt;document_content&gt;
MIT License

Copyright (c) 2022 Andrej Karpathy

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;4&quot;&gt;
&lt;source&gt;README.md&lt;/source&gt;
&lt;document_content&gt;

# nanoGPT

![nanoGPT](assets/nanogpt.jpg)

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#x27;s it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](https://pytorch.org) &lt;3
- [numpy](https://numpy.org/install/) &lt;3
-  `transformers` for huggingface transformers &lt;3 (to load GPT-2 checkpoints)
-  `datasets` for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)
-  `tiktoken` for OpenAI&#x27;s fast BPE code &lt;3
-  `wandb` for optional logging &lt;3
-  `tqdm` for progress bars &lt;3

## quick start

If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:

```sh
python data/shakespeare_char/prepare.py
```

This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:

**I have a GPU**. Great, we can quickly train a baby GPT with the settings provided in the [config/train_shakespeare_char.py](config/train_shakespeare_char.py) config file:

```sh
python train.py config/train_shakespeare_char.py
```

If you peek inside it, you&#x27;ll see that we&#x27;re training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-shakespeare-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:

```sh
python sample.py --out_dir=out-shakespeare-char
```

This generates a few samples, for example:

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang&#x27;d
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

lol  `Â¯\_(ãƒ„)_/Â¯`. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).

**I only have a macbook** (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly ([select it here](https://pytorch.org/get-started/locally/) when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:

```sh
python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```

Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We&#x27;ll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it&#x27;s still good fun:

```sh
python sample.py --out_dir=out-shakespeare-char --device=cpu
```
Generates samples like this:

```
GLEORKEN VINGHARD III:
Whell&#x27;s the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
```

Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you&#x27;re willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.

Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for &quot;Metal Performance Shaders&quot;); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.

## reproducing GPT-2

A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/), an open reproduction of OpenAI&#x27;s (private) WebText:

```sh
python data/openwebtext/prepare.py
```

This downloads and tokenizes the [OpenWebText](https://huggingface.co/datasets/openwebtext) dataset. It will create a `train.bin` and `val.bin` which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#x27;re ready to kick off training. To reproduce GPT-2 (124M) you&#x27;ll want at least an 8X A100 40GB node and run:

```sh
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.

If you&#x27;re in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:

```sh
# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
```

It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don&#x27;t have Infiniband then also prepend `NCCL_IB_DISABLE=1` to the above launches. Your multinode training will work, but most likely _crawl_. By default checkpoints are periodically written to the `--out_dir`. We can sample from the model by simply `python sample.py`.

Finally, to train on a single GPU simply run the `python train.py` script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You&#x27;ll most likely want to tune a number of those variables depending on your needs.

## baselines

OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:

```sh
$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
```

and observe the following losses on train and val:

| model | params | train loss | val loss |
| ------| ------ | ---------- | -------- |
| gpt2 | 124M         | 3.11  | 3.12     |
| gpt2-medium | 350M  | 2.85  | 2.84     |
| gpt2-large | 774M   | 2.66  | 2.67     |
| gpt2-xl | 1558M     | 2.56  | 2.54     |

However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.

## finetuning

Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to `data/shakespeare` and run `prepare.py` to download the tiny shakespeare dataset and render it into a `train.bin` and `val.bin`, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:

```sh
python train.py config/finetune_shakespeare.py
```

This will load the config parameter overrides in `config/finetune_shakespeare.py` (I didn&#x27;t tune them much though). Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. If you&#x27;re running out of memory try decreasing the model size (they are `{&#x27;gpt2&#x27;, &#x27;gpt2-medium&#x27;, &#x27;gpt2-large&#x27;, &#x27;gpt2-xl&#x27;}`) or possibly decreasing the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-shakespeare` by default, per the config file. You can then run the code in `sample.py --out_dir=out-shakespeare`:

```
THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know&#x27;st not what thou sell&#x27;st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
```

Whoa there, GPT, entering some dark place over there. I didn&#x27;t really tune the hyperparameters in the config too much, feel free to try!

## sampling / inference

Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:

```sh
python sample.py \
    --init_from=gpt2-xl \
    --start=&quot;What is the answer to life, the universe, and everything?&quot; \
    --num_samples=5 --max_new_tokens=100
```

If you&#x27;d like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g. ```python sample.py --start=FILE:prompt.txt```.

## efficiency notes

For simple model benchmarking and profiling, `bench.py` might be useful. It&#x27;s identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.

Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!

## todos

- Investigate and add FSDP instead of DDP
- Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)
- Finetune the finetuning script, I think the hyperparams are not great
- Schedule for linear batch size increase during training
- Incorporate other embeddings (rotary, alibi)
- Separate out the optim buffers from model params in checkpoints I think
- Additional logging around network health (e.g. gradient clip events, magnitudes)
- Few more investigations around better init etc.

## troubleshooting

Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you&#x27;re running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.

For some context on this repository, GPT, and language modeling it might be helpful to watch my [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.

For more questions/discussions feel free to stop by **#nanoGPT** on Discord:

[![](https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat)](https://discord.gg/3zy8kqD9Cp)

## acknowledgements

All nanoGPT experiments are powered by GPUs on [Lambda labs](https://lambdalabs.com), my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;5&quot;&gt;
&lt;source&gt;bench.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
A much shorter version of train.py for benchmarking
&quot;&quot;&quot;
import os
from contextlib import nullcontext
import numpy as np
import time
import torch
from model import GPTConfig, GPT

# -----------------------------------------------------------------------------
batch_size = 12
block_size = 1024
bias = False
real_data = True
seed = 1337
device = &#x27;cuda&#x27; # examples: &#x27;cpu&#x27;, &#x27;cuda&#x27;, &#x27;cuda:0&#x27;, &#x27;cuda:1&#x27;, etc.
dtype = &#x27;bfloat16&#x27; if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else &#x27;float16&#x27; # &#x27;float32&#x27; or &#x27;bfloat16&#x27; or &#x27;float16&#x27;
compile = True # use PyTorch 2.0 to compile the model to be faster
profile = False # use pytorch profiler, or just simple benchmarking?
exec(open(&#x27;configurator.py&#x27;).read()) # overrides from command line or config file
# -----------------------------------------------------------------------------

torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = &#x27;cuda&#x27; if &#x27;cuda&#x27; in device else &#x27;cpu&#x27; # for later use in torch.autocast
ptdtype = {&#x27;float32&#x27;: torch.float32, &#x27;bfloat16&#x27;: torch.bfloat16, &#x27;float16&#x27;: torch.float16}[dtype]
ctx = nullcontext() if device_type == &#x27;cpu&#x27; else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# data loading init
if real_data:
    dataset = &#x27;openwebtext&#x27;
    data_dir = os.path.join(&#x27;data&#x27;, dataset)
    train_data = np.memmap(os.path.join(data_dir, &#x27;train.bin&#x27;), dtype=np.uint16, mode=&#x27;r&#x27;)
    def get_batch(split):
        data = train_data # note ignore split in benchmarking script
        ix = torch.randint(len(data) - block_size, (batch_size,))
        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
        return x, y
else:
    # alternatively, if fixed data is desired to not care about data loading
    x = torch.randint(50304, (batch_size, block_size), device=device)
    y = torch.randint(50304, (batch_size, block_size), device=device)
    get_batch = lambda split: (x, y)

# model init
gptconf = GPTConfig(
    block_size = block_size, # how far back does the model look? i.e. context size
    n_layer = 12, n_head = 12, n_embd = 768, # size of the model
    dropout = 0, # for determinism
    bias = bias,
)
model = GPT(gptconf)
model.to(device)

optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)

if compile:
    print(&quot;Compiling model...&quot;)
    model = torch.compile(model) # pytorch 2.0

if profile:
    # useful docs on pytorch profiler:
    # - tutorial https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html
    # - api https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile
    wait, warmup, active = 5, 5, 5
    num_steps = wait + warmup + active
    with torch.profiler.profile(
        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
        schedule=torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1),
        on_trace_ready=torch.profiler.tensorboard_trace_handler(&#x27;./bench_log&#x27;),
        record_shapes=False,
        profile_memory=False,
        with_stack=False, # incurs an additional overhead, disable if not needed
        with_flops=True,
        with_modules=False, # only for torchscript models atm
    ) as prof:

        X, Y = get_batch(&#x27;train&#x27;)
        for k in range(num_steps):
            with ctx:
                logits, loss = model(X, Y)
            X, Y = get_batch(&#x27;train&#x27;)
            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()
            lossf = loss.item()
            print(f&quot;{k}/{num_steps} loss: {lossf:.4f}&quot;)

            prof.step() # notify the profiler at end of each step

else:

    # simple benchmarking
    torch.cuda.synchronize()
    for stage, num_steps in enumerate([10, 20]): # burnin, then benchmark
        t0 = time.time()
        X, Y = get_batch(&#x27;train&#x27;)
        for k in range(num_steps):
            with ctx:
                logits, loss = model(X, Y)
            X, Y = get_batch(&#x27;train&#x27;)
            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()
            lossf = loss.item()
            print(f&quot;{k}/{num_steps} loss: {lossf:.4f}&quot;)
        torch.cuda.synchronize()
        t1 = time.time()
        dt = t1-t0
        mfu = model.estimate_mfu(batch_size * 1 * num_steps, dt)
        if stage == 1:
            print(f&quot;time per iteration: {dt/num_steps*1000:.4f}ms, MFU: {mfu*100:.2f}%&quot;)

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;6&quot;&gt;
&lt;source&gt;config/eval_gpt2.py&lt;/source&gt;
&lt;document_content&gt;
# evaluate the base gpt2
# n_layer=12, n_head=12, n_embd=768
# 124M parameters
batch_size = 8
eval_iters = 500 # use more iterations to get good estimate
eval_only = True
wandb_log = False
init_from = &#x27;gpt2&#x27;

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;7&quot;&gt;
&lt;source&gt;config/eval_gpt2_large.py&lt;/source&gt;
&lt;document_content&gt;
# evaluate the base gpt2
# n_layer=36, n_head=20, n_embd=1280
# 774M parameters
batch_size = 8
eval_iters = 500 # use more iterations to get good estimate
eval_only = True
wandb_log = False
init_from = &#x27;gpt2-large&#x27;

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;8&quot;&gt;
&lt;source&gt;config/eval_gpt2_medium.py&lt;/source&gt;
&lt;document_content&gt;
# evaluate the base gpt2
# n_layer=24, n_head=16, n_embd=1024
# 350M parameters
batch_size = 8
eval_iters = 500 # use more iterations to get good estimate
eval_only = True
wandb_log = False
init_from = &#x27;gpt2-medium&#x27;

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;9&quot;&gt;
&lt;source&gt;config/eval_gpt2_xl.py&lt;/source&gt;
&lt;document_content&gt;
# evaluate the base gpt2
# n_layer=48, n_head=25, n_embd=1600
# 1558M parameters
batch_size = 8
eval_iters = 500 # use more iterations to get good estimate
eval_only = True
wandb_log = False
init_from = &#x27;gpt2-xl&#x27;

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;10&quot;&gt;
&lt;source&gt;config/finetune_shakespeare.py&lt;/source&gt;
&lt;document_content&gt;
import time

out_dir = &#x27;out-shakespeare&#x27;
eval_interval = 5
eval_iters = 40
wandb_log = False # feel free to turn on
wandb_project = &#x27;shakespeare&#x27;
wandb_run_name = &#x27;ft-&#x27; + str(time.time())

dataset = &#x27;shakespeare&#x27;
init_from = &#x27;gpt2-xl&#x27; # this is the largest GPT-2 model

# only save checkpoints if the validation loss improves
always_save_checkpoint = False

# the number of examples per iter:
# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter
# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters
batch_size = 1
gradient_accumulation_steps = 32
max_iters = 20

# finetune at constant LR
learning_rate = 3e-5
decay_lr = False

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;11&quot;&gt;
&lt;source&gt;config/train_gpt2.py&lt;/source&gt;
&lt;document_content&gt;
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

wandb_log = True
wandb_project = &#x27;owt&#x27;
wandb_run_name=&#x27;gpt2-124M&#x27;

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;12&quot;&gt;
&lt;source&gt;config/train_shakespeare_char.py&lt;/source&gt;
&lt;document_content&gt;
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = &#x27;out-shakespeare-char&#x27;
eval_interval = 250 # keep frequent because we&#x27;ll overfit
eval_iters = 200
log_interval = 10 # don&#x27;t print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = &#x27;shakespeare-char&#x27;
wandb_run_name = &#x27;mini-gpt&#x27;

dataset = &#x27;shakespeare_char&#x27;
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = &#x27;cpu&#x27;  # run on cpu only
# compile = False # do not torch compile the model

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;13&quot;&gt;
&lt;source&gt;configurator.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
Poor Man&#x27;s Configurator. Probably a terrible idea. Example usage:
$ python train.py config/override_file.py --batch_size=32
this will first run config/override_file.py, then override batch_size to 32

The code in this file will be run as follows from e.g. train.py:
&gt;&gt;&gt; exec(open(&#x27;configurator.py&#x27;).read())

So it&#x27;s not a Python module, it&#x27;s just shuttling this code away from train.py
The code in this script then overrides the globals()

I know people are not going to love this, I just really dislike configuration
complexity and having to prepend config. to every single variable. If someone
comes up with a better simple Python solution I am all ears.
&quot;&quot;&quot;

import sys
from ast import literal_eval

for arg in sys.argv[1:]:
    if &#x27;=&#x27; not in arg:
        # assume it&#x27;s the name of a config file
        assert not arg.startswith(&#x27;--&#x27;)
        config_file = arg
        print(f&quot;Overriding config with {config_file}:&quot;)
        with open(config_file) as f:
            print(f.read())
        exec(open(config_file).read())
    else:
        # assume it&#x27;s a --key=value argument
        assert arg.startswith(&#x27;--&#x27;)
        key, val = arg.split(&#x27;=&#x27;)
        key = key[2:]
        if key in globals():
            try:
                # attempt to eval it it (e.g. if bool, number, or etc)
                attempt = literal_eval(val)
            except (SyntaxError, ValueError):
                # if that goes wrong, just use the string
                attempt = val
            # ensure the types match ok
            assert type(attempt) == type(globals()[key])
            # cross fingers
            print(f&quot;Overriding: {key} = {attempt}&quot;)
            globals()[key] = attempt
        else:
            raise ValueError(f&quot;Unknown config key: {key}&quot;)

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;14&quot;&gt;
&lt;source&gt;data/openwebtext/prepare.py&lt;/source&gt;
&lt;document_content&gt;
# saves the openwebtext dataset to a binary file for training. following was helpful:
# https://github.com/HazyResearch/flash-attention/blob/main/training/src/datamodules/language_modeling_hf.py

import os
from tqdm import tqdm
import numpy as np
import tiktoken
from datasets import load_dataset # huggingface datasets

# number of workers in .map() call
# good number to use is ~order number of cpu cores // 2
num_proc = 8

# number of workers in load_dataset() call
# best number might be different from num_proc above as it also depends on NW speed.
# it is better than 1 usually though
num_proc_load_dataset = num_proc

enc = tiktoken.get_encoding(&quot;gpt2&quot;)

if __name__ == &#x27;__main__&#x27;:
    # takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)
    dataset = load_dataset(&quot;openwebtext&quot;, num_proc=num_proc_load_dataset)

    # owt by default only contains the &#x27;train&#x27; split, so create a test split
    split_dataset = dataset[&quot;train&quot;].train_test_split(test_size=0.0005, seed=2357, shuffle=True)
    split_dataset[&#x27;val&#x27;] = split_dataset.pop(&#x27;test&#x27;) # rename the test split to val

    # this results in:
    # &gt;&gt;&gt; split_dataset
    # DatasetDict({
    #     train: Dataset({
    #         features: [&#x27;text&#x27;],
    #         num_rows: 8009762
    #     })
    #     val: Dataset({
    #         features: [&#x27;text&#x27;],
    #         num_rows: 4007
    #     })
    # })

    # we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)
    def process(example):
        ids = enc.encode_ordinary(example[&#x27;text&#x27;]) # encode_ordinary ignores any special tokens
        ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe
        # note: I think eot should be prepended not appended... hmm. it&#x27;s called &quot;eot&quot; though...
        out = {&#x27;ids&#x27;: ids, &#x27;len&#x27;: len(ids)}
        return out

    # tokenize the dataset
    tokenized = split_dataset.map(
        process,
        remove_columns=[&#x27;text&#x27;],
        desc=&quot;tokenizing the splits&quot;,
        num_proc=num_proc,
    )

    # concatenate all the ids in each dataset into one large file we can use for training
    for split, dset in tokenized.items():
        arr_len = np.sum(dset[&#x27;len&#x27;], dtype=np.uint64)
        filename = os.path.join(os.path.dirname(__file__), f&#x27;{split}.bin&#x27;)
        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is &lt; 2**16)
        arr = np.memmap(filename, dtype=dtype, mode=&#x27;w+&#x27;, shape=(arr_len,))
        total_batches = 1024

        idx = 0
        for batch_idx in tqdm(range(total_batches), desc=f&#x27;writing {filename}&#x27;):
            # Batch together samples for faster write
            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format(&#x27;numpy&#x27;)
            arr_batch = np.concatenate(batch[&#x27;ids&#x27;])
            # Write into mmap
            arr[idx : idx + len(arr_batch)] = arr_batch
            idx += len(arr_batch)
        arr.flush()

    # train.bin is ~17GB, val.bin ~8.5MB
    # train has ~9B tokens (9,035,582,198)
    # val has ~4M tokens (4,434,897)

    # to read the bin files later, e.g. with numpy:
    # m = np.memmap(&#x27;train.bin&#x27;, dtype=np.uint16, mode=&#x27;r&#x27;)

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;15&quot;&gt;
&lt;source&gt;data/openwebtext/readme.md&lt;/source&gt;
&lt;document_content&gt;

## openwebtext dataset

after running `prepare.py` (preprocess) we get:

- train.bin is ~17GB, val.bin ~8.5MB
- train has ~9B tokens (9,035,582,198)
- val has ~4M tokens (4,434,897)

this came from 8,013,769 documents in total.

references:

- OpenAI&#x27;s WebText dataset is discussed in [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;16&quot;&gt;
&lt;source&gt;data/shakespeare/prepare.py&lt;/source&gt;
&lt;document_content&gt;
import os
import requests
import tiktoken
import numpy as np

# download the tiny shakespeare dataset
input_file_path = os.path.join(os.path.dirname(__file__), &#x27;input.txt&#x27;)
if not os.path.exists(input_file_path):
    data_url = &#x27;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#x27;
    with open(input_file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:
        f.write(requests.get(data_url).text)

with open(input_file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:
    data = f.read()
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# encode with tiktoken gpt2 bpe
enc = tiktoken.get_encoding(&quot;gpt2&quot;)
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)
print(f&quot;train has {len(train_ids):,} tokens&quot;)
print(f&quot;val has {len(val_ids):,} tokens&quot;)

# export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(os.path.dirname(__file__), &#x27;train.bin&#x27;))
val_ids.tofile(os.path.join(os.path.dirname(__file__), &#x27;val.bin&#x27;))

# train.bin has 301,966 tokens
# val.bin has 36,059 tokens

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;17&quot;&gt;
&lt;source&gt;data/shakespeare/readme.md&lt;/source&gt;
&lt;document_content&gt;

# tiny shakespeare

Tiny shakespeare, of the good old char-rnn fame :)

After running `prepare.py`:

- train.bin has 301,966 tokens
- val.bin has 36,059 tokens

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;18&quot;&gt;
&lt;source&gt;data/shakespeare_char/prepare.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
Prepare the Shakespeare dataset for character-level language modeling.
So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.
Will save train.bin, val.bin containing the ids, and meta.pkl containing the
encoder and decoder and some other related info.
&quot;&quot;&quot;
import os
import pickle
import requests
import numpy as np

# download the tiny shakespeare dataset
input_file_path = os.path.join(os.path.dirname(__file__), &#x27;input.txt&#x27;)
if not os.path.exists(input_file_path):
    data_url = &#x27;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#x27;
    with open(input_file_path, &#x27;w&#x27;) as f:
        f.write(requests.get(data_url).text)

with open(input_file_path, &#x27;r&#x27;) as f:
    data = f.read()
print(f&quot;length of dataset in characters: {len(data):,}&quot;)

# get all the unique characters that occur in this text
chars = sorted(list(set(data)))
vocab_size = len(chars)
print(&quot;all the unique characters:&quot;, &#x27;&#x27;.join(chars))
print(f&quot;vocab size: {vocab_size:,}&quot;)

# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
def encode(s):
    return [stoi[c] for c in s] # encoder: take a string, output a list of integers
def decode(l):
    return &#x27;&#x27;.join([itos[i] for i in l]) # decoder: take a list of integers, output a string

# create the train and test splits
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# encode both to integers
train_ids = encode(train_data)
val_ids = encode(val_data)
print(f&quot;train has {len(train_ids):,} tokens&quot;)
print(f&quot;val has {len(val_ids):,} tokens&quot;)

# export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(os.path.dirname(__file__), &#x27;train.bin&#x27;))
val_ids.tofile(os.path.join(os.path.dirname(__file__), &#x27;val.bin&#x27;))

# save the meta information as well, to help us encode/decode later
meta = {
    &#x27;vocab_size&#x27;: vocab_size,
    &#x27;itos&#x27;: itos,
    &#x27;stoi&#x27;: stoi,
}
with open(os.path.join(os.path.dirname(__file__), &#x27;meta.pkl&#x27;), &#x27;wb&#x27;) as f:
    pickle.dump(meta, f)

# length of dataset in characters:  1115394
# all the unique characters:
#  !$&amp;&#x27;,-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
# vocab size: 65
# train has 1003854 tokens
# val has 111540 tokens

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;19&quot;&gt;
&lt;source&gt;data/shakespeare_char/readme.md&lt;/source&gt;
&lt;document_content&gt;

# tiny shakespeare, character-level

Tiny shakespeare, of the good old char-rnn fame :) Treated on character-level.

After running `prepare.py`:

- train.bin has 1,003,854 tokens
- val.bin has 111,540 tokens

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;20&quot;&gt;
&lt;source&gt;model.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
Full definition of a GPT Language Model, all of it in this single file.
References:
1) the official GPT-2 TensorFlow implementation released by OpenAI:
https://github.com/openai/gpt-2/blob/master/src/model.py
2) huggingface/transformers PyTorch implementation:
https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
&quot;&quot;&quot;

import math
import inspect
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.nn import functional as F

class LayerNorm(nn.Module):
    &quot;&quot;&quot; LayerNorm but with an optional bias. PyTorch doesn&#x27;t support simply bias=False &quot;&quot;&quot;

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, &#x27;scaled_dot_product_attention&#x27;)
        if not self.flash:
            print(&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;)
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(&quot;bias&quot;, torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(&#x27;-inf&#x27;))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # with weight tying when using torch.compile() some warnings get generated:
        # &quot;UserWarning: functional_call was passed multiple values for tied weights.
        # This behavior is deprecated and will be an error in future versions&quot;
        # not 100% sure what this is, so far seems to be harmless. TODO investigate
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

        # init all weights
        self.apply(self._init_weights)
        # apply special scaled init to the residual projections, per GPT-2 paper
        for pn, p in self.named_parameters():
            if pn.endswith(&#x27;c_proj.weight&#x27;):
                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))

        # report number of parameters
        print(&quot;number of parameters: %.2fM&quot; % (self.get_num_params()/1e6,))

    def get_num_params(self, non_embedding=True):
        &quot;&quot;&quot;
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        &quot;&quot;&quot;
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t &lt;= self.config.block_size, f&quot;Cannot forward sequence of length {t}, block size is only {self.config.block_size}&quot;
        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)

        # forward the GPT model itself
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
        x = self.transformer.drop(tok_emb + pos_emb)
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            loss = None

        return logits, loss

    def crop_block_size(self, block_size):
        # model surgery to decrease the block size if necessary
        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
        # but want to use a smaller block size for some smaller, simpler model
        assert block_size &lt;= self.config.block_size
        self.config.block_size = block_size
        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
        for block in self.transformer.h:
            if hasattr(block.attn, &#x27;bias&#x27;):
                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]

    @classmethod
    def from_pretrained(cls, model_type, override_args=None):
        assert model_type in {&#x27;gpt2&#x27;, &#x27;gpt2-medium&#x27;, &#x27;gpt2-large&#x27;, &#x27;gpt2-xl&#x27;}
        override_args = override_args or {} # default to empty dict
        # only dropout can be overridden see more notes below
        assert all(k == &#x27;dropout&#x27; for k in override_args)
        from transformers import GPT2LMHeadModel
        print(&quot;loading weights from pretrained gpt: %s&quot; % model_type)

        # n_layer, n_head and n_embd are determined from model_type
        config_args = {
            &#x27;gpt2&#x27;:         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
            &#x27;gpt2-medium&#x27;:  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
            &#x27;gpt2-large&#x27;:   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
            &#x27;gpt2-xl&#x27;:      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
        }[model_type]
        print(&quot;forcing vocab_size=50257, block_size=1024, bias=True&quot;)
        config_args[&#x27;vocab_size&#x27;] = 50257 # always 50257 for GPT model checkpoints
        config_args[&#x27;block_size&#x27;] = 1024 # always 1024 for GPT model checkpoints
        config_args[&#x27;bias&#x27;] = True # always True for GPT model checkpoints
        # we can override the dropout rate, if desired
        if &#x27;dropout&#x27; in override_args:
            print(f&quot;overriding dropout rate to {override_args[&#x27;dropout&#x27;]}&quot;)
            config_args[&#x27;dropout&#x27;] = override_args[&#x27;dropout&#x27;]
        # create a from-scratch initialized minGPT model
        config = GPTConfig(**config_args)
        model = GPT(config)
        sd = model.state_dict()
        sd_keys = sd.keys()
        sd_keys = [k for k in sd_keys if not k.endswith(&#x27;.attn.bias&#x27;)] # discard this mask / buffer, not a param

        # init a huggingface/transformers model
        model_hf = GPT2LMHeadModel.from_pretrained(model_type)
        sd_hf = model_hf.state_dict()

        # copy while ensuring all of the parameters are aligned and match in names and shapes
        sd_keys_hf = sd_hf.keys()
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(&#x27;.attn.masked_bias&#x27;)] # ignore these, just a buffer
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(&#x27;.attn.bias&#x27;)] # same, just the mask (buffer)
        transposed = [&#x27;attn.c_attn.weight&#x27;, &#x27;attn.c_proj.weight&#x27;, &#x27;mlp.c_fc.weight&#x27;, &#x27;mlp.c_proj.weight&#x27;]
        # basically the openai checkpoints use a &quot;Conv1D&quot; module, but we only want to use a vanilla Linear
        # this means that we have to transpose these weights when we import them
        assert len(sd_keys_hf) == len(sd_keys), f&quot;mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}&quot;
        for k in sd_keys_hf:
            if any(k.endswith(w) for w in transposed):
                # special treatment for the Conv1D weights we need to transpose
                assert sd_hf[k].shape[::-1] == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k].t())
            else:
                # vanilla copy over the other parameters
                assert sd_hf[k].shape == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k])

        return model

    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
        # start with all of the candidate parameters
        param_dict = {pn: p for pn, p in self.named_parameters()}
        # filter out those that do not require grad
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#x27;t.
        decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]
        optim_groups = [
            {&#x27;params&#x27;: decay_params, &#x27;weight_decay&#x27;: weight_decay},
            {&#x27;params&#x27;: nodecay_params, &#x27;weight_decay&#x27;: 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f&quot;num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters&quot;)
        print(f&quot;num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters&quot;)
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = &#x27;fused&#x27; in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and device_type == &#x27;cuda&#x27;
        extra_args = dict(fused=True) if use_fused else dict()
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
        print(f&quot;using fused AdamW: {use_fused}&quot;)

        return optimizer

    def estimate_mfu(self, fwdbwd_per_iter, dt):
        &quot;&quot;&quot; estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS &quot;&quot;&quot;
        # first estimate the number of flops we do per iteration.
        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
        N = self.get_num_params()
        cfg = self.config
        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
        flops_per_token = 6*N + 12*L*H*Q*T
        flops_per_fwdbwd = flops_per_token * T
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        # express our flops throughput as ratio of A100 bfloat16 peak flops
        flops_achieved = flops_per_iter * (1.0/dt) # per second
        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
        mfu = flops_achieved / flops_promised
        return mfu

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        &quot;&quot;&quot;
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you&#x27;ll want to make sure to be in model.eval() mode of operation for this.
        &quot;&quot;&quot;
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, _ = self(idx_cond)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits &lt; v[:, [-1]]] = -float(&#x27;Inf&#x27;)
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;21&quot;&gt;
&lt;source&gt;sample.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
Sample from a trained model
&quot;&quot;&quot;
import os
import pickle
from contextlib import nullcontext
import torch
import tiktoken
from model import GPTConfig, GPT

# -----------------------------------------------------------------------------
init_from = &#x27;resume&#x27; # either &#x27;resume&#x27; (from an out_dir) or a gpt2 variant (e.g. &#x27;gpt2-xl&#x27;)
out_dir = &#x27;out&#x27; # ignored if init_from is not &#x27;resume&#x27;
start = &quot;\n&quot; # or &quot;&lt;|endoftext|&gt;&quot; or etc. Can also specify a file, use as: &quot;FILE:prompt.txt&quot;
num_samples = 10 # number of samples to draw
max_new_tokens = 500 # number of tokens generated in each sample
temperature = 0.8 # 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions
top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability
seed = 1337
device = &#x27;cuda&#x27; # examples: &#x27;cpu&#x27;, &#x27;cuda&#x27;, &#x27;cuda:0&#x27;, &#x27;cuda:1&#x27;, etc.
dtype = &#x27;bfloat16&#x27; if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else &#x27;float16&#x27; # &#x27;float32&#x27; or &#x27;bfloat16&#x27; or &#x27;float16&#x27;
compile = False # use PyTorch 2.0 to compile the model to be faster
exec(open(&#x27;configurator.py&#x27;).read()) # overrides from command line or config file
# -----------------------------------------------------------------------------

torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = &#x27;cuda&#x27; if &#x27;cuda&#x27; in device else &#x27;cpu&#x27; # for later use in torch.autocast
ptdtype = {&#x27;float32&#x27;: torch.float32, &#x27;bfloat16&#x27;: torch.bfloat16, &#x27;float16&#x27;: torch.float16}[dtype]
ctx = nullcontext() if device_type == &#x27;cpu&#x27; else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# model
if init_from == &#x27;resume&#x27;:
    # init from a model saved in a specific directory
    ckpt_path = os.path.join(out_dir, &#x27;ckpt.pt&#x27;)
    checkpoint = torch.load(ckpt_path, map_location=device)
    gptconf = GPTConfig(**checkpoint[&#x27;model_args&#x27;])
    model = GPT(gptconf)
    state_dict = checkpoint[&#x27;model&#x27;]
    unwanted_prefix = &#x27;_orig_mod.&#x27;
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
elif init_from.startswith(&#x27;gpt2&#x27;):
    # init from a given GPT-2 model
    model = GPT.from_pretrained(init_from, dict(dropout=0.0))

model.eval()
model.to(device)
if compile:
    model = torch.compile(model) # requires PyTorch 2.0 (optional)

# look for the meta pickle in case it is available in the dataset folder
load_meta = False
if init_from == &#x27;resume&#x27; and &#x27;config&#x27; in checkpoint and &#x27;dataset&#x27; in checkpoint[&#x27;config&#x27;]: # older checkpoints might not have these...
    meta_path = os.path.join(&#x27;data&#x27;, checkpoint[&#x27;config&#x27;][&#x27;dataset&#x27;], &#x27;meta.pkl&#x27;)
    load_meta = os.path.exists(meta_path)
if load_meta:
    print(f&quot;Loading meta from {meta_path}...&quot;)
    with open(meta_path, &#x27;rb&#x27;) as f:
        meta = pickle.load(f)
    # TODO want to make this more general to arbitrary encoder/decoder schemes
    stoi, itos = meta[&#x27;stoi&#x27;], meta[&#x27;itos&#x27;]
    encode = lambda s: [stoi[c] for c in s]
    decode = lambda l: &#x27;&#x27;.join([itos[i] for i in l])
else:
    # ok let&#x27;s assume gpt-2 encodings by default
    print(&quot;No meta.pkl found, assuming GPT-2 encodings...&quot;)
    enc = tiktoken.get_encoding(&quot;gpt2&quot;)
    encode = lambda s: enc.encode(s, allowed_special={&quot;&lt;|endoftext|&gt;&quot;})
    decode = lambda l: enc.decode(l)

# encode the beginning of the prompt
if start.startswith(&#x27;FILE:&#x27;):
    with open(start[5:], &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:
        start = f.read()
start_ids = encode(start)
x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])

# run generation
with torch.no_grad():
    with ctx:
        for k in range(num_samples):
            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
            print(decode(y[0].tolist()))
            print(&#x27;---------------&#x27;)

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;22&quot;&gt;
&lt;source&gt;train.py&lt;/source&gt;
&lt;document_content&gt;
&quot;&quot;&quot;
This training script can be run both on a single gpu in debug mode,
and also in a larger training run with distributed data parallel (ddp).

To run on a single GPU, example:
$ python train.py --batch_size=32 --compile=False

To run with DDP on 4 gpus on 1 node, example:
$ torchrun --standalone --nproc_per_node=4 train.py

To run with DDP on 4 gpus across 2 nodes, example:
- Run on the first (master) node with example IP 123.456.123.456:
$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
- Run on the worker node:
$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)
&quot;&quot;&quot;

import os
import time
import math
import pickle
from contextlib import nullcontext

import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group

from model import GPTConfig, GPT

# -----------------------------------------------------------------------------
# default config values designed to train a gpt2 (124M) on OpenWebText
# I/O
out_dir = &#x27;out&#x27;
eval_interval = 2000
log_interval = 1
eval_iters = 200
eval_only = False # if True, script exits right after the first eval
always_save_checkpoint = True # if True, always save a checkpoint after each eval
init_from = &#x27;scratch&#x27; # &#x27;scratch&#x27; or &#x27;resume&#x27; or &#x27;gpt2*&#x27;
# wandb logging
wandb_log = False # disabled by default
wandb_project = &#x27;owt&#x27;
wandb_run_name = &#x27;gpt2&#x27; # &#x27;run&#x27; + str(time.time())
# data
dataset = &#x27;openwebtext&#x27;
gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes
batch_size = 12 # if gradient_accumulation_steps &gt; 1, this is the micro-batch size
block_size = 1024
# model
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias = False # do we use bias inside LayerNorm and Linear layers?
# adamw optimizer
learning_rate = 6e-4 # max learning rate
max_iters = 600000 # total number of training iterations
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # whether to decay the learning rate
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
# DDP settings
backend = &#x27;nccl&#x27; # &#x27;nccl&#x27;, &#x27;gloo&#x27;, etc.
# system
device = &#x27;cuda&#x27; # examples: &#x27;cpu&#x27;, &#x27;cuda&#x27;, &#x27;cuda:0&#x27;, &#x27;cuda:1&#x27; etc., or try &#x27;mps&#x27; on macbooks
dtype = &#x27;bfloat16&#x27; if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else &#x27;float16&#x27; # &#x27;float32&#x27;, &#x27;bfloat16&#x27;, or &#x27;float16&#x27;, the latter will auto implement a GradScaler
compile = True # use PyTorch 2.0 to compile the model to be faster
# -----------------------------------------------------------------------------
config_keys = [k for k,v in globals().items() if not k.startswith(&#x27;_&#x27;) and isinstance(v, (int, float, bool, str))]
exec(open(&#x27;configurator.py&#x27;).read()) # overrides from command line or config file
config = {k: globals()[k] for k in config_keys} # will be useful for logging
# -----------------------------------------------------------------------------

# various inits, derived attributes, I/O setup
ddp = int(os.environ.get(&#x27;RANK&#x27;, -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ[&#x27;RANK&#x27;])
    ddp_local_rank = int(os.environ[&#x27;LOCAL_RANK&#x27;])
    ddp_world_size = int(os.environ[&#x27;WORLD_SIZE&#x27;])
    device = f&#x27;cuda:{ddp_local_rank}&#x27;
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # if not ddp, we are running on a single gpu, and one process
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print(f&quot;tokens per iteration will be: {tokens_per_iter:,}&quot;)

if master_process:
    os.makedirs(out_dir, exist_ok=True)
torch.manual_seed(1337 + seed_offset)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = &#x27;cuda&#x27; if &#x27;cuda&#x27; in device else &#x27;cpu&#x27; # for later use in torch.autocast
# note: float16 data type will automatically use a GradScaler
ptdtype = {&#x27;float32&#x27;: torch.float32, &#x27;bfloat16&#x27;: torch.bfloat16, &#x27;float16&#x27;: torch.float16}[dtype]
ctx = nullcontext() if device_type == &#x27;cpu&#x27; else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# poor man&#x27;s data loader
data_dir = os.path.join(&#x27;data&#x27;, dataset)
def get_batch(split):
    # We recreate np.memmap every batch to avoid a memory leak, as per
    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122
    if split == &#x27;train&#x27;:
        data = np.memmap(os.path.join(data_dir, &#x27;train.bin&#x27;), dtype=np.uint16, mode=&#x27;r&#x27;)
    else:
        data = np.memmap(os.path.join(data_dir, &#x27;val.bin&#x27;), dtype=np.uint16, mode=&#x27;r&#x27;)
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    if device_type == &#x27;cuda&#x27;:
        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x, y

# init these up here, can override if init_from=&#x27;resume&#x27; (i.e. from a checkpoint)
iter_num = 0
best_val_loss = 1e9

# attempt to derive vocab_size from the dataset
meta_path = os.path.join(data_dir, &#x27;meta.pkl&#x27;)
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, &#x27;rb&#x27;) as f:
        meta = pickle.load(f)
    meta_vocab_size = meta[&#x27;vocab_size&#x27;]
    print(f&quot;found vocab_size = {meta_vocab_size} (inside {meta_path})&quot;)

# model init
model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line
if init_from == &#x27;scratch&#x27;:
    # init a new model from scratch
    print(&quot;Initializing a new model from scratch&quot;)
    # determine the vocab size we&#x27;ll use for from-scratch training
    if meta_vocab_size is None:
        print(&quot;defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)&quot;)
    model_args[&#x27;vocab_size&#x27;] = meta_vocab_size if meta_vocab_size is not None else 50304
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
elif init_from == &#x27;resume&#x27;:
    print(f&quot;Resuming training from {out_dir}&quot;)
    # resume training from a checkpoint.
    ckpt_path = os.path.join(out_dir, &#x27;ckpt.pt&#x27;)
    checkpoint = torch.load(ckpt_path, map_location=device)
    checkpoint_model_args = checkpoint[&#x27;model_args&#x27;]
    # force these config attributes to be equal otherwise we can&#x27;t even resume training
    # the rest of the attributes (e.g. dropout) can stay as desired from command line
    for k in [&#x27;n_layer&#x27;, &#x27;n_head&#x27;, &#x27;n_embd&#x27;, &#x27;block_size&#x27;, &#x27;bias&#x27;, &#x27;vocab_size&#x27;]:
        model_args[k] = checkpoint_model_args[k]
    # create the model
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
    state_dict = checkpoint[&#x27;model&#x27;]
    # fix the keys of the state dictionary :(
    # honestly no idea how checkpoints sometimes get this prefix, have to debug more
    unwanted_prefix = &#x27;_orig_mod.&#x27;
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    iter_num = checkpoint[&#x27;iter_num&#x27;]
    best_val_loss = checkpoint[&#x27;best_val_loss&#x27;]
elif init_from.startswith(&#x27;gpt2&#x27;):
    print(f&quot;Initializing from OpenAI GPT-2 weights: {init_from}&quot;)
    # initialize from OpenAI GPT-2 weights
    override_args = dict(dropout=dropout)
    model = GPT.from_pretrained(init_from, override_args)
    # read off the created config params, so we can store them into checkpoint correctly
    for k in [&#x27;n_layer&#x27;, &#x27;n_head&#x27;, &#x27;n_embd&#x27;, &#x27;block_size&#x27;, &#x27;bias&#x27;, &#x27;vocab_size&#x27;]:
        model_args[k] = getattr(model.config, k)
# crop down the model block size if desired, using model surgery
if block_size &lt; model.config.block_size:
    model.crop_block_size(block_size)
    model_args[&#x27;block_size&#x27;] = block_size # so that the checkpoint will have the right value
model.to(device)

# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == &#x27;float16&#x27;))

# optimizer
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
if init_from == &#x27;resume&#x27;:
    optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;])
checkpoint = None # free up memory

# compile the model
if compile:
    print(&quot;compiling the model... (takes a ~minute)&quot;)
    unoptimized_model = model
    model = torch.compile(model) # requires PyTorch 2.0

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])

# helps estimate an arbitrarily accurate loss over either split using many batches
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in [&#x27;train&#x27;, &#x27;val&#x27;]:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            with ctx:
                logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# learning rate decay scheduler (cosine with warmup)
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it &lt; warmup_iters:
        return learning_rate * (it + 1) / (warmup_iters + 1)
    # 2) if it &gt; lr_decay_iters, return min learning rate
    if it &gt; lr_decay_iters:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    assert 0 &lt;= decay_ratio &lt;= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1
    return min_lr + coeff * (learning_rate - min_lr)

# logging
if wandb_log and master_process:
    import wandb
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)

# training loop
X, Y = get_batch(&#x27;train&#x27;) # fetch the very first batch
t0 = time.time()
local_iter_num = 0 # number of iterations in the lifetime of this process
raw_model = model.module if ddp else model # unwrap DDP container if needed
running_mfu = -1.0
while True:

    # determine and set the learning rate for this iteration
    lr = get_lr(iter_num) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group[&#x27;lr&#x27;] = lr

    # evaluate the loss on train/val sets and write checkpoints
    if iter_num % eval_interval == 0 and master_process:
        losses = estimate_loss()
        print(f&quot;step {iter_num}: train loss {losses[&#x27;train&#x27;]:.4f}, val loss {losses[&#x27;val&#x27;]:.4f}&quot;)
        if wandb_log:
            wandb.log({
                &quot;iter&quot;: iter_num,
                &quot;train/loss&quot;: losses[&#x27;train&#x27;],
                &quot;val/loss&quot;: losses[&#x27;val&#x27;],
                &quot;lr&quot;: lr,
                &quot;mfu&quot;: running_mfu*100, # convert to percentage
            })
        if losses[&#x27;val&#x27;] &lt; best_val_loss or always_save_checkpoint:
            best_val_loss = losses[&#x27;val&#x27;]
            if iter_num &gt; 0:
                checkpoint = {
                    &#x27;model&#x27;: raw_model.state_dict(),
                    &#x27;optimizer&#x27;: optimizer.state_dict(),
                    &#x27;model_args&#x27;: model_args,
                    &#x27;iter_num&#x27;: iter_num,
                    &#x27;best_val_loss&#x27;: best_val_loss,
                    &#x27;config&#x27;: config,
                }
                print(f&quot;saving checkpoint to {out_dir}&quot;)
                torch.save(checkpoint, os.path.join(out_dir, &#x27;ckpt.pt&#x27;))
    if iter_num == 0 and eval_only:
        break

    # forward backward update, with optional gradient accumulation to simulate larger batch size
    # and using the GradScaler if data type is float16
    for micro_step in range(gradient_accumulation_steps):
        if ddp:
            # in DDP training we only need to sync gradients at the last micro step.
            # the official way to do this is with model.no_sync() context manager, but
            # I really dislike that this bloats the code and forces us to repeat code
            # looking at the source of that context manager, it just toggles this variable
            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
        with ctx:
            logits, loss = model(X, Y)
            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation
        # immediately async prefetch next batch while model is doing the forward pass on the GPU
        X, Y = get_batch(&#x27;train&#x27;)
        # backward pass, with gradient scaling if training in fp16
        scaler.scale(loss).backward()
    # clip the gradient
    if grad_clip != 0.0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
    # step the optimizer and scaler if training in fp16
    scaler.step(optimizer)
    scaler.update()
    # flush the gradients as soon as we can, no need for this memory anymore
    optimizer.zero_grad(set_to_none=True)

    # timing and logging
    t1 = time.time()
    dt = t1 - t0
    t0 = t1
    if iter_num % log_interval == 0 and master_process:
        # get loss as float. note: this is a CPU-GPU sync point
        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)
        lossf = loss.item() * gradient_accumulation_steps
        if local_iter_num &gt;= 5: # let the training loop settle a bit
            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu
        print(f&quot;iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%&quot;)
    iter_num += 1
    local_iter_num += 1

    # termination conditions
    if iter_num &gt; max_iters:
        break

if ddp:
    destroy_process_group()

&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;23&quot;&gt;
&lt;source&gt;transformer_sizing.ipynb&lt;/source&gt;
&lt;document_content&gt;
{
 &quot;cells&quot;: [
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### Transformer Theoretical Model\n&quot;,
    &quot;\n&quot;,
    &quot;This notebook stores a bunch of analysis about a Transformer, e.g. estimates the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 1,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from collections import OrderedDict&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 2,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# config_args = {\n&quot;,
    &quot;#     &#x27;gpt2&#x27;:         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n&quot;,
    &quot;#     &#x27;gpt2-medium&#x27;:  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n&quot;,
    &quot;#     &#x27;gpt2-large&#x27;:   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n&quot;,
    &quot;#     &#x27;gpt2-xl&#x27;:      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n&quot;,
    &quot;# }[model_type]\n&quot;,
    &quot;\n&quot;,
    &quot;block_size = 1024\n&quot;,
    &quot;vocab_size = 50257\n&quot;,
    &quot;n_layer = 12\n&quot;,
    &quot;n_head = 12\n&quot;,
    &quot;n_embd = 768\n&quot;,
    &quot;bias = False\n&quot;,
    &quot;assert not bias, \&quot;this notebook assumes bias=False just for simplicity\&quot;&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 3,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;we see: 124337664, expected: 124337664, match: True\n&quot;,
      &quot;name                 params     ratio (%) \n&quot;,
      &quot;emebedding/position      786432     0.6325\n&quot;,
      &quot;embedding/token        38597376    31.0424\n&quot;,
      &quot;embedding              39383808    31.6749\n&quot;,
      &quot;attention/ln                768     0.0006\n&quot;,
      &quot;attention/kqv           1769472     1.4231\n&quot;,
      &quot;attention/proj           589824     0.4744\n&quot;,
      &quot;attention               2360064     1.8981\n&quot;,
      &quot;mlp/ln                      768     0.0006\n&quot;,
      &quot;mlp/ffw                 2359296     1.8975\n&quot;,
      &quot;mlp/proj                2359296     1.8975\n&quot;,
      &quot;mlp                     4719360     3.7956\n&quot;,
      &quot;block                   7079424     5.6937\n&quot;,
      &quot;transformer            84953088    68.3245\n&quot;,
      &quot;ln_f                        768     0.0006\n&quot;,
      &quot;dense                         0     0.0000\n&quot;,
      &quot;total                 124337664   100.0000\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;def params():\n&quot;,
    &quot;    \&quot;\&quot;\&quot; estimates the number of parameters in the model\&quot;\&quot;\&quot;\n&quot;,
    &quot;    out = OrderedDict()\n&quot;,
    &quot;\n&quot;,
    &quot;    # token and position embeddings\n&quot;,
    &quot;    out[&#x27;emebedding/position&#x27;] = n_embd * block_size\n&quot;,
    &quot;    out[&#x27;embedding/token&#x27;] = n_embd * vocab_size\n&quot;,
    &quot;    out[&#x27;embedding&#x27;] = out[&#x27;emebedding/position&#x27;] + out[&#x27;embedding/token&#x27;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # attention blocks\n&quot;,
    &quot;    out[&#x27;attention/ln&#x27;] = n_embd # note, bias=False in our LN\n&quot;,
    &quot;    out[&#x27;attention/kqv&#x27;] = n_embd * 3*n_embd\n&quot;,
    &quot;    out[&#x27;attention/proj&#x27;] = n_embd**2\n&quot;,
    &quot;    out[&#x27;attention&#x27;] = out[&#x27;attention/ln&#x27;] + out[&#x27;attention/kqv&#x27;] + out[&#x27;attention/proj&#x27;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # MLP blocks\n&quot;,
    &quot;    ffw_size = 4*n_embd # feed forward size\n&quot;,
    &quot;    out[&#x27;mlp/ln&#x27;] = n_embd\n&quot;,
    &quot;    out[&#x27;mlp/ffw&#x27;] = n_embd * ffw_size\n&quot;,
    &quot;    out[&#x27;mlp/proj&#x27;] = ffw_size * n_embd\n&quot;,
    &quot;    out[&#x27;mlp&#x27;] = out[&#x27;mlp/ln&#x27;] + out[&#x27;mlp/ffw&#x27;] + out[&#x27;mlp/proj&#x27;]\n&quot;,
    &quot;    \n&quot;,
    &quot;    # the transformer and the rest of it\n&quot;,
    &quot;    out[&#x27;block&#x27;] = out[&#x27;attention&#x27;] + out[&#x27;mlp&#x27;]\n&quot;,
    &quot;    out[&#x27;transformer&#x27;] = n_layer * out[&#x27;block&#x27;]\n&quot;,
    &quot;    out[&#x27;ln_f&#x27;] = n_embd # final layernorm\n&quot;,
    &quot;    out[&#x27;dense&#x27;] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n&quot;,
    &quot;\n&quot;,
    &quot;    # total\n&quot;,
    &quot;    out[&#x27;total&#x27;] = out[&#x27;embedding&#x27;] + out[&#x27;transformer&#x27;] + out[&#x27;ln_f&#x27;] + out[&#x27;dense&#x27;]\n&quot;,
    &quot;\n&quot;,
    &quot;    return out\n&quot;,
    &quot;\n&quot;,
    &quot;# compare our param count to that reported by PyTorch\n&quot;,
    &quot;p = params()\n&quot;,
    &quot;params_total = p[&#x27;total&#x27;]\n&quot;,
    &quot;print(f\&quot;we see: {params_total}, expected: {124337664}, match: {params_total == 124337664}\&quot;)\n&quot;,
    &quot;# create a header\n&quot;,
    &quot;print(f\&quot;{&#x27;name&#x27;:20s} {&#x27;params&#x27;:10s} {&#x27;ratio (%)&#x27;:10s}\&quot;)\n&quot;,
    &quot;for k,v in p.items():\n&quot;,
    &quot;    print(f\&quot;{k:20s} {v:10d} {v/params_total*100:10.4f}\&quot;)\n&quot;,
    &quot;    &quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 4,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;est checkpoint size: 1.49 GB\n&quot;,
      &quot;measured with wc -c ckpt.pt: 1542470366\n&quot;,
      &quot;fluff ratio: 103.38%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# we can now calculate the size of each checkpoint\n&quot;,
    &quot;# params are stored in fp32, and the AdamW optimizer has 2 additional buffers per param for statistics\n&quot;,
    &quot;params_bytes = params_total*4\n&quot;,
    &quot;params_and_buffers_bytes = params_bytes + 2*params_bytes\n&quot;,
    &quot;print(f\&quot;est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\&quot;)\n&quot;,
    &quot;measured_bytes = 1542470366 # from wc -c ckpt.pt\n&quot;,
    &quot;print(f\&quot;measured with wc -c ckpt.pt: {measured_bytes}\&quot;)\n&quot;,
    &quot;print(f\&quot;fluff ratio: {measured_bytes/params_and_buffers_bytes*100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We can also estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 5,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;memory ratio taken up just for parameters: 3.73%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;gpu_memory = 40e9 # 40 GB A100 GPU, roughly\n&quot;,
    &quot;print(f\&quot;memory ratio taken up just for parameters: {params_and_buffers_bytes / gpu_memory * 100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;i.e. not that much of the memory for this tiny model, most of the memory is activations (forward and backward). This of course changes dramatically for larger and larger models.&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Let&#x27;s estimate FLOPs for a single forward pass.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 6,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;name                 flops          ratio (%) \n&quot;,
      &quot;attention/kqv            3623878656     1.2426\n&quot;,
      &quot;attention/scores         1610612736     0.5522\n&quot;,
      &quot;attention/reduce         1610612736     0.5522\n&quot;,
      &quot;attention/proj           1207959552     0.4142\n&quot;,
      &quot;attention                8053063680     2.7612\n&quot;,
      &quot;mlp/ffw1                 4831838208     1.6567\n&quot;,
      &quot;mlp/ffw2                 4831838208     1.6567\n&quot;,
      &quot;mlp                      9663676416     3.3135\n&quot;,
      &quot;block                   17716740096     6.0747\n&quot;,
      &quot;transformer            212600881152    72.8963\n&quot;,
      &quot;dense                   79047426048    27.1037\n&quot;,
      &quot;forward_total          291648307200   100.0000\n&quot;,
      &quot;backward_total         583296614400   200.0000\n&quot;,
      &quot;total                  874944921600   300.0000\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;def flops():\n&quot;,
    &quot;    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n&quot;,
    &quot;    # we count actual FLOPs, not MACs. Hence 2* all over the place\n&quot;,
    &quot;    # basically for any matrix multiply A (BxC) @ B (CxD) -&gt; (BxD) flops are 2*B*C*D\n&quot;,
    &quot;\n&quot;,
    &quot;    out = OrderedDict()\n&quot;,
    &quot;    head_size = n_embd // n_head\n&quot;,
    &quot;\n&quot;,
    &quot;    # attention blocks\n&quot;,
    &quot;    # 1) the projection to key, query, values\n&quot;,
    &quot;    out[&#x27;attention/kqv&#x27;] = 2 * block_size * (n_embd * 3*n_embd)\n&quot;,
    &quot;    # 2) calculating the attention scores\n&quot;,
    &quot;    out[&#x27;attention/scores&#x27;] = 2 * block_size * block_size * n_embd\n&quot;,
    &quot;    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n&quot;,
    &quot;    out[&#x27;attention/reduce&#x27;] = 2 * n_head * (block_size * block_size * head_size)\n&quot;,
    &quot;    # 4) the final linear projection\n&quot;,
    &quot;    out[&#x27;attention/proj&#x27;] = 2 * block_size * (n_embd * n_embd)\n&quot;,
    &quot;    out[&#x27;attention&#x27;] = sum(out[&#x27;attention/&#x27;+k] for k in [&#x27;kqv&#x27;, &#x27;scores&#x27;, &#x27;reduce&#x27;, &#x27;proj&#x27;])\n&quot;,
    &quot;\n&quot;,
    &quot;    # MLP blocks\n&quot;,
    &quot;    ffw_size = 4*n_embd # feed forward size\n&quot;,
    &quot;    out[&#x27;mlp/ffw1&#x27;] = 2 * block_size * (n_embd * ffw_size)\n&quot;,
    &quot;    out[&#x27;mlp/ffw2&#x27;] = 2 * block_size * (ffw_size * n_embd)\n&quot;,
    &quot;    out[&#x27;mlp&#x27;] = out[&#x27;mlp/ffw1&#x27;] + out[&#x27;mlp/ffw2&#x27;]\n&quot;,
    &quot;\n&quot;,
    &quot;    # the transformer and the rest of it\n&quot;,
    &quot;    out[&#x27;block&#x27;] = out[&#x27;attention&#x27;] + out[&#x27;mlp&#x27;]\n&quot;,
    &quot;    out[&#x27;transformer&#x27;] = n_layer * out[&#x27;block&#x27;]\n&quot;,
    &quot;    out[&#x27;dense&#x27;] = 2 * block_size * (n_embd * vocab_size)\n&quot;,
    &quot;\n&quot;,
    &quot;    # forward,backward,total\n&quot;,
    &quot;    out[&#x27;forward_total&#x27;] = out[&#x27;transformer&#x27;] + out[&#x27;dense&#x27;]\n&quot;,
    &quot;    out[&#x27;backward_total&#x27;] = 2 * out[&#x27;forward_total&#x27;] # use common estimate of bwd = 2*fwd\n&quot;,
    &quot;    out[&#x27;total&#x27;] = out[&#x27;forward_total&#x27;] + out[&#x27;backward_total&#x27;]\n&quot;,
    &quot;\n&quot;,
    &quot;    return out\n&quot;,
    &quot;    \n&quot;,
    &quot;# compare our param count to that reported by PyTorch\n&quot;,
    &quot;f = flops()\n&quot;,
    &quot;flops_total = f[&#x27;forward_total&#x27;]\n&quot;,
    &quot;print(f\&quot;{&#x27;name&#x27;:20s} {&#x27;flops&#x27;:14s} {&#x27;ratio (%)&#x27;:10s}\&quot;)\n&quot;,
    &quot;for k,v in f.items():\n&quot;,
    &quot;    print(f\&quot;{k:20s} {v:14d} {v/flops_total*100:10.4f}\&quot;)\n&quot;,
    &quot;    &quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 7,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;palm_flops: 875062886400, flops: 874944921600, ratio: 1.0001\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# now here is an estimate copy pasted from the PaLM paper\n&quot;,
    &quot;# this formula is often used to calculate MFU (model flops utilization)\n&quot;,
    &quot;def palm_flops():\n&quot;,
    &quot;    \&quot;\&quot;\&quot;estimate of the model flops following PaLM paper formula\&quot;\&quot;\&quot;\n&quot;,
    &quot;    # non-embedding model parameters. note that we do not subtract the\n&quot;,
    &quot;    # embedding/token params because those are tied and get used in the last layer.\n&quot;,
    &quot;    N = params()[&#x27;total&#x27;] - params()[&#x27;emebedding/position&#x27;]\n&quot;,
    &quot;    L, H, Q, T = n_layer, n_head, n_embd//n_head, block_size\n&quot;,
    &quot;    mf_per_token = 6*N + 12*L*H*Q*T\n&quot;,
    &quot;    mf = mf_per_token * block_size\n&quot;,
    &quot;    return mf\n&quot;,
    &quot;\n&quot;,
    &quot;print(f\&quot;palm_flops: {palm_flops():d}, flops: {flops()[&#x27;total&#x27;]:d}, ratio: {palm_flops()/flops()[&#x27;total&#x27;]:.4f}\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Ok they are quite similar, giving some confidence that my math in flops() function was ~ok. Now, A100 is cited at 312TFLOPS bfloat16 on tensor cores. So what is our model flops utilization (MFU)? I trained the model above with a batch_size of 20 and grad_accum of 5, which runs in about 755ms on a single A100 GPU. We get:&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 8,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;fraction of A100 used: 37.14%\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# here is what we currently roughly measure\n&quot;,
    &quot;batch_size = 20 * 5 # 5 is grad_accum, so total batch size is 100\n&quot;,
    &quot;measured_time = 0.755 # in seconds per iteration\n&quot;,
    &quot;measured_throughput = batch_size / measured_time\n&quot;,
    &quot;flops_achieved = f[&#x27;total&#x27;] * measured_throughput\n&quot;,
    &quot;\n&quot;,
    &quot;# A100 is cited to be 312 TFLOPS of bloat16 running on tensor cores\n&quot;,
    &quot;a100_flops_promised = 312e12\n&quot;,
    &quot;\n&quot;,
    &quot;# the fraction of the A100 that we are using:\n&quot;,
    &quot;print(f\&quot;fraction of A100 used: {flops_achieved / a100_flops_promised * 100:.2f}%\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;For reference, we&#x27;d prefer to be somewhere around 50%+, and not just for a single GPU but for an entire DDP run. So we still have some work to do, but at least we&#x27;re within a factor of ~2X of what is achievable with this GPU.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 9,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;time needed to train the model: 3.46 days\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;# Finally let&#x27;s check out the 6ND approximation as total cost of training in FLOPs\n&quot;,
    &quot;model_size = params()[&#x27;total&#x27;] # this is number of parameters, N\n&quot;,
    &quot;tokens_num = 300e9 # 300B tokens, this is dataset size in tokens, D\n&quot;,
    &quot;a100_flops = 312e12 # 312 TFLOPS\n&quot;,
    &quot;assumed_mfu = 0.3 # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n&quot;,
    &quot;flops_throughput = a100_flops * 8 * assumed_mfu # assume an 8XA100 node at 30% utilization\n&quot;,
    &quot;flops_needed = 6 * model_size * tokens_num # 6ND\n&quot;,
    &quot;time_needed_s = flops_needed / flops_throughput # in seconds\n&quot;,
    &quot;print(f\&quot;time needed to train the model: {time_needed_s/3600/24:.2f} days\&quot;)&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;This is not a bad estimate at all. I trained this model and it converged in roughly 4 days. Btw as a good reference for where 6ND comes from and some intuition around it I recommend [Dzmitry&#x27;s post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4).&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Now, FLOPs are just one constraint, the other that we have to keep a close track of is the memory bandwidth. TODO estimate LOAD/STORE costs of our model later.&quot;
   ]
  }
 ],
 &quot;metadata&quot;: {
  &quot;kernelspec&quot;: {
   &quot;display_name&quot;: &quot;pytorch2&quot;,
   &quot;language&quot;: &quot;python&quot;,
   &quot;name&quot;: &quot;python3&quot;
  },
  &quot;language_info&quot;: {
   &quot;codemirror_mode&quot;: {
    &quot;name&quot;: &quot;ipython&quot;,
    &quot;version&quot;: 3
   },
   &quot;file_extension&quot;: &quot;.py&quot;,
   &quot;mimetype&quot;: &quot;text/x-python&quot;,
   &quot;name&quot;: &quot;python&quot;,
   &quot;nbconvert_exporter&quot;: &quot;python&quot;,
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
   &quot;version&quot;: &quot;3.10.8&quot;
  },
  &quot;orig_nbformat&quot;: 4,
  &quot;vscode&quot;: {
   &quot;interpreter&quot;: {
    &quot;hash&quot;: &quot;7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222&quot;
   }
  }
 },
 &quot;nbformat&quot;: 4,
 &quot;nbformat_minor&quot;: 2
}

&lt;/document_content&gt;
&lt;/document&gt;
&lt;/documents&gt;</textarea>
        <div class="copy-hint">
          ğŸ’¡ <strong>Pro tip:</strong> Click in the text area above and use <kbd>Ctrl+A</kbd> (or <kbd>Cmd+A</kbd> on Mac) to select all content, then <kbd>Ctrl+C</kbd> (or <kbd>Cmd+C</kbd>) to copy to clipboard.
        </div>
      </div>
    </div>
  </main>
</div>

<script>
// Mobile sidebar functionality
function toggleSidebar() {
  const sidebar = document.getElementById('sidebar');
  const overlay = document.querySelector('.sidebar-overlay');
  
  if (sidebar && overlay) {
    sidebar.classList.toggle('open');
    overlay.classList.toggle('show');
  }
}

function closeSidebar() {
  const sidebar = document.getElementById('sidebar');
  const overlay = document.querySelector('.sidebar-overlay');
  
  if (sidebar && overlay) {
    sidebar.classList.remove('open');
    overlay.classList.remove('show');
  }
}

// Enhanced view switching with smooth transitions
function showHumanView(buttonElement) {
  const humanView = document.getElementById('human-view');
  const llmView = document.getElementById('llm-view');
  const toggleBtns = document.querySelectorAll('.toggle-btn');
  
  if (!humanView || !llmView) return;
  
  // Update button states first
  toggleBtns.forEach(btn => btn.classList.remove('active'));
  if (buttonElement) {
    buttonElement.classList.add('active');
  } else {
    document.querySelector('.toggle-btn:first-of-type').classList.add('active');
  }
  
  // Fade out current view
  llmView.style.opacity = '0';
  llmView.style.transform = 'translateY(20px)';
  
  setTimeout(() => {
    llmView.style.display = 'none';
    humanView.style.display = 'block';
    humanView.style.opacity = '0';
    humanView.style.transform = 'translateY(20px)';
    
    // Fade in new view
    requestAnimationFrame(() => {
      humanView.style.transition = 'all 0.3s ease';
      humanView.style.opacity = '1';
      humanView.style.transform = 'translateY(0)';
    });
  }, 150);
}

function showLLMView(buttonElement) {
  const humanView = document.getElementById('human-view');
  const llmView = document.getElementById('llm-view');
  const toggleBtns = document.querySelectorAll('.toggle-btn');
  
  if (!humanView || !llmView) return;
  
  // Update button states first
  toggleBtns.forEach(btn => btn.classList.remove('active'));
  if (buttonElement) {
    buttonElement.classList.add('active');
  } else {
    document.querySelector('.toggle-btn:last-of-type').classList.add('active');
  }
  
  // Fade out current view
  humanView.style.opacity = '0';
  humanView.style.transform = 'translateY(20px)';
  
  setTimeout(() => {
    humanView.style.display = 'none';
    llmView.style.display = 'block';
    llmView.style.opacity = '0';
    llmView.style.transform = 'translateY(20px)';
    
    // Fade in new view
    requestAnimationFrame(() => {
      llmView.style.transition = 'all 0.3s ease';
      llmView.style.opacity = '1';
      llmView.style.transform = 'translateY(0)';
    });
    
    // Auto-select all text when switching to LLM view for easy copying
    setTimeout(() => {
      const textArea = document.getElementById('llm-text');
      if (textArea) {
        textArea.focus();
        textArea.select();
      }
    }, 300);
  }, 150);
}

// Smooth scrolling for anchor links
document.addEventListener('DOMContentLoaded', function() {
  // Add smooth scrolling to all anchor links
  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
      e.preventDefault();
      const target = document.querySelector(this.getAttribute('href'));
      if (target) {
        target.scrollIntoView({
          behavior: 'smooth',
          block: 'start'
        });
        
        // Close sidebar on mobile after navigation
        if (window.innerWidth <= 768) {
          closeSidebar();
        }
      }
    });
  });
  
  // Add loading animation
  document.body.style.opacity = '0';
  requestAnimationFrame(() => {
    document.body.style.transition = 'opacity 0.5s ease';
    document.body.style.opacity = '1';
  });
  
  // Add intersection observer for fade-in animations
  const observer = new IntersectionObserver((entries) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        entry.target.style.opacity = '1';
        entry.target.style.transform = 'translateY(0)';
      }
    });
  }, { threshold: 0.1 });
  
  // Observe file sections for fade-in effect
  document.querySelectorAll('.file-section').forEach(section => {
    section.style.opacity = '0';
    section.style.transform = 'translateY(30px)';
    section.style.transition = 'all 0.6s ease';
    observer.observe(section);
  });
  
  // Close sidebar when clicking outside on mobile
  document.addEventListener('click', function(e) {
    if (window.innerWidth <= 768) {
      const sidebar = document.getElementById('sidebar');
      const mobileNavBtn = document.querySelector('.mobile-nav-btn');
      
      if (sidebar && sidebar.classList.contains('open') && 
          !sidebar.contains(e.target) && 
          !mobileNavBtn.contains(e.target)) {
        closeSidebar();
      }
    }
  });
  
  // Handle window resize
  window.addEventListener('resize', function() {
    if (window.innerWidth > 768) {
      closeSidebar();
    }
  });
  
  // Add copy to clipboard functionality for code blocks
  document.querySelectorAll('.file-section').forEach(fileSection => {
    const codeBlock = fileSection.querySelector('.highlight');
    if (!codeBlock) return;
    
    // Check if copy button already exists
    if (fileSection.querySelector('.copy-code-btn')) return;
    
    const copyBtn = document.createElement('button');
    copyBtn.textContent = 'ğŸ“‹ Copy';
    copyBtn.className = 'copy-code-btn';
    
    const header = fileSection.querySelector('h2');
    if (header && codeBlock) {
      header.appendChild(copyBtn);
      
      copyBtn.addEventListener('click', (e) => {
        e.preventDefault();
        e.stopPropagation();
        const code = codeBlock.textContent || '';
        
        if (navigator.clipboard && navigator.clipboard.writeText) {
          navigator.clipboard.writeText(code).then(() => {
            copyBtn.textContent = 'âœ… Copied!';
            copyBtn.style.background = 'var(--success-gradient)';
            setTimeout(() => {
              copyBtn.textContent = 'ğŸ“‹ Copy';
              copyBtn.style.background = 'var(--primary-gradient)';
            }, 2000);
          }).catch(() => {
            // Fallback for clipboard API failure
            fallbackCopy(code, copyBtn);
          });
        } else {
          // Fallback for browsers without clipboard API
          fallbackCopy(code, copyBtn);
        }
      });
    }
  });
  
  // Fallback copy function
  function fallbackCopy(text, button) {
    const textArea = document.createElement('textarea');
    textArea.value = text;
    textArea.style.position = 'fixed';
    textArea.style.left = '-999999px';
    textArea.style.top = '-999999px';
    document.body.appendChild(textArea);
    textArea.focus();
    textArea.select();
    
    try {
      document.execCommand('copy');
      button.textContent = 'âœ… Copied!';
      button.style.background = 'var(--success-gradient)';
    } catch (err) {
      button.textContent = 'âŒ Failed';
      button.style.background = 'var(--danger-gradient)';
    }
    
    document.body.removeChild(textArea);
    setTimeout(() => {
      button.textContent = 'ğŸ“‹ Copy';
      button.style.background = 'var(--primary-gradient)';
    }, 2000);
  }
});

// Add keyboard shortcuts
document.addEventListener('keydown', function(e) {
  // Alt + 1 for Human view
  if (e.altKey && e.key === '1') {
    e.preventDefault();
    const btn = document.querySelector('.toggle-btn:first-of-type');
    if (btn) showHumanView(btn);
  }
  
  // Alt + 2 for LLM view
  if (e.altKey && e.key === '2') {
    e.preventDefault();
    const btn = document.querySelector('.toggle-btn:last-of-type');
    if (btn) showLLMView(btn);
  }
  
  // Ctrl/Cmd + K to focus search (if we add it later)
  if ((e.ctrlKey || e.metaKey) && e.key === 'k') {
    e.preventDefault();
    // Focus search functionality could be added here
  }
});
</script>


    <script>
    // Advanced search functionality
    function initSearchFeatures() {
      // Add search box to sidebar
      const sidebar = document.getElementById('sidebar');
      const sidebarInner = sidebar.querySelector('.sidebar-inner');
      
      const searchHTML = `
        <div class="search-container" style="margin-bottom: 1.5rem;">
          <div class="search-box">
            <input type="text" id="file-search" placeholder="ğŸ” Search files..." 
                   style="width: 100%; padding: 0.75rem 1rem; border: 2px solid var(--border-light); 
                          border-radius: var(--radius-md); font-size: 0.9rem; background: white;
                          transition: all 0.2s ease;">
            <div id="search-results" style="margin-top: 0.5rem; display: none;"></div>
          </div>
          <div class="search-stats" style="font-size: 0.8rem; color: var(--text-tertiary); 
                                         margin-top: 0.5rem; text-align: center;"></div>
        </div>`;
      
      sidebarInner.insertAdjacentHTML('afterbegin', searchHTML);
      
      const searchInput = document.getElementById('file-search');
      const searchResults = document.getElementById('search-results');
      const searchStats = document.querySelector('.search-stats');
      const allFiles = Array.from(document.querySelectorAll('.toc-file a'));
      
      searchInput.addEventListener('input', (e) => {
        const query = e.target.value.toLowerCase().trim();
        
        if (query.length < 2) {
          searchResults.style.display = 'none';
          resetFileVisibility();
          searchStats.textContent = '';
          return;
        }
        
        const matches = allFiles.filter(link => {
          const filename = link.textContent.toLowerCase();
          return filename.includes(query);
        });
        
        // Show/hide files in TOC based on search
        allFiles.forEach(link => {
          const listItem = link.closest('.toc-file');
          if (matches.includes(link)) {
            listItem.style.display = 'list-item';
            // Highlight matching text
            const text = link.innerHTML;
            const regex = new RegExp(`(${query})`, 'gi');
            link.innerHTML = text.replace(regex, '<mark style="background: yellow; padding: 0.1em;">$1</mark>');
          } else {
            listItem.style.display = 'none';
          }
        });
        
        searchStats.textContent = `${matches.length} file(s) match "${query}"`;
        
        if (matches.length === 0) {
          searchResults.innerHTML = '<div style="padding: 0.5rem; color: var(--text-tertiary); font-style: italic;">No matching files found</div>';
          searchResults.style.display = 'block';
        } else {
          searchResults.style.display = 'none';
        }
      });
      
      function resetFileVisibility() {
        allFiles.forEach(link => {
          link.closest('.toc-file').style.display = 'list-item';
          // Remove highlighting
          const text = link.textContent;
          link.innerHTML = link.innerHTML.replace(/<mark[^>]*>([^<]*)<\/mark>/gi, '$1');
        });
      }
    }

    // Add breadcrumb navigation
    function addBreadcrumbs() {
      document.querySelectorAll('.file-section h2').forEach(header => {
        const filePath = header.textContent.split('(')[0].trim();
        const pathParts = filePath.split('/');
        
        if (pathParts.length > 1) {
          const breadcrumb = document.createElement('div');
          breadcrumb.className = 'breadcrumb';
          breadcrumb.style.cssText = `
            font-size: 0.85rem; color: var(--text-tertiary); margin-bottom: 0.5rem;
            font-family: 'JetBrains Mono', monospace;
          `;
          
          breadcrumb.innerHTML = pathParts.map((part, index) => {
            if (index === pathParts.length - 1) {
              return `<span style="color: var(--text-primary); font-weight: 600;">${part}</span>`;
            } else {
              return `<span>${part}</span>`;
            }
          }).join(' <span style="color: var(--text-tertiary);">â†’</span> ');
          
          header.parentNode.insertBefore(breadcrumb, header);
        }
      });
    }

    // Add line numbers
    function addLineNumbers() {
      document.querySelectorAll('.highlight pre').forEach(pre => {
        const code = pre.textContent;
        const lines = code.split('\n');
        const lineNumbers = lines.map((_, i) => i + 1).join('\n');
        
        const lineNumbersEl = document.createElement('div');
        lineNumbersEl.style.cssText = `
          position: absolute; left: 0; top: 0; bottom: 0; width: 3rem;
          background: rgba(0,0,0,0.1); border-right: 1px solid rgba(255,255,255,0.1);
          font-family: 'JetBrains Mono', monospace; font-size: 0.75rem;
          color: rgba(255,255,255,0.5); text-align: right; padding: 1.5rem 0.5rem;
          line-height: 1.5; user-select: none; white-space: pre;
        `;
        lineNumbersEl.textContent = lineNumbers;
        
        pre.parentElement.style.position = 'relative';
        pre.style.paddingLeft = '4rem';
        pre.parentElement.insertBefore(lineNumbersEl, pre);
      });
    }

    // File content analysis
    function addContentAnalysis() {
      document.querySelectorAll('.file-section').forEach(section => {
        const codeBlock = section.querySelector('.highlight, .markdown-content');
        if (!codeBlock) return;
        
        const content = codeBlock.textContent || '';
        const lines = content.split('\n').length;
        const words = content.split(/\s+/).length;
        const chars = content.length;
        
        const analysisEl = document.createElement('div');
        analysisEl.className = 'content-analysis';
        analysisEl.style.cssText = `
          background: var(--bg-tertiary); padding: 0.75rem 1rem; 
          border-radius: var(--radius-sm); margin-bottom: 1rem;
          font-size: 0.85rem; color: var(--text-secondary);
          display: flex; gap: 1rem; flex-wrap: wrap;
        `;
        
        analysisEl.innerHTML = `
          <span>ğŸ“ <strong>${lines}</strong> lines</span>
          <span>ğŸ“ <strong>${words}</strong> words</span>
          <span>ğŸ”¤ <strong>${chars.toLocaleString()}</strong> chars</span>
          <span>â±ï¸ ~<strong>${Math.ceil(words / 200)}</strong> min read</span>
        `;
        
        const fileBody = section.querySelector('.file-body');
        fileBody.insertBefore(analysisEl, fileBody.firstChild);
      });
    }

    // Export functions
    function exportToPDF() {
      // Simple PDF export using browser print
      const originalTitle = document.title;
      document.title = 'Repository Export - ' + originalTitle;
      window.print();
      document.title = originalTitle;
    }
    
    function copyShareableLink() {
      navigator.clipboard.writeText(window.location.href).then(() => {
        showToast('âœ… Link copied to clipboard!');
      });
    }
    
    function downloadHTML() {
      const htmlContent = document.documentElement.outerHTML;
      const blob = new Blob([htmlContent], { type: 'text/html' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = 'repository-export.html';
      a.click();
      URL.revokeObjectURL(url);
    }
    
    function generateQR() {
      // Simple QR code generation using online service
      const qrUrl = `https://api.qrserver.com/v1/create-qr-code/?size=200x200&data=${encodeURIComponent(window.location.href)}`;
      const modal = document.createElement('div');
      modal.style.cssText = `
        position: fixed; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0,0,0,0.8);
        display: flex; align-items: center; justify-content: center; z-index: 10000;
      `;
      modal.innerHTML = `
        <div style="background: white; padding: 2rem; border-radius: var(--radius-lg); text-align: center;">
          <h3>ğŸ“± QR Code for this page</h3>
          <img src="${qrUrl}" alt="QR Code" style="margin: 1rem 0;"/>
          <br><button onclick="this.parentElement.parentElement.remove()" 
                     style="padding: 0.5rem 1rem; background: var(--primary-gradient); color: white; border: none; border-radius: var(--radius-sm);">Close</button>
        </div>
      `;
      document.body.appendChild(modal);
    }
    
    function showToast(message) {
      const toast = document.createElement('div');
      toast.textContent = message;
      toast.style.cssText = `
        position: fixed; top: 2rem; right: 2rem; background: var(--success-gradient);
        color: white; padding: 1rem 1.5rem; border-radius: var(--radius-md);
        box-shadow: var(--shadow-lg); z-index: 10000; animation: slideIn 0.3s ease;
      `;
      document.body.appendChild(toast);
      setTimeout(() => toast.remove(), 3000);
    }

    // Initialize all interactive features
    document.addEventListener('DOMContentLoaded', () => {
      setTimeout(() => {
        addLineNumbers();
        addContentAnalysis();
        initSearchFeatures();
        addBreadcrumbs();
      }, 500);
    });
    </script>
    
</body>
</html>
